

@inproceedings{RH:86,
  author =        {R. Hamming},
  booktitle =     {Bell Communications Research Colloquium Seminar},
  note =          {Transcripted by James F. Kaiser. (Later version of
                   the talk at
                   \url{https://www.youtube.com/watch?v=a1zDuOPkMSw})},
  organization =  {Bell Communications Research, Morristown, NJ},
  pages =         {483},
  title =         {You and your research},
  year =          {1986},
  url =           {http://motion.me.ucsb.edu/RHamming-YouAndYourResearch-
                  1986.pdf},
}

@book{JH-AK-RGP:91,
  author =        {J. Hertz and A. Krogh and R. G. Palmer},
  publisher =     {CRC Press},
  title =         {Introduction to the Theory of Neural Computation},
  year =          {1991},
}

@book{KG:97,
  author =        {K. Gurney},
  publisher =     {{CRC} Press},
  title =         {An Introduction to Neural Networks},
  year =          {1997},
  doi =           {10.1201/9781315273570},
}

@book{PD-LFA:05,
  author =        {P. Dayan and L. F. Abbott},
  publisher =     {MIT Press},
  title =         {Theoretical Neuroscience: Computational and
                   Mathematical Modeling of Neural Systems},
  year =          {2005},
}

@book{ACCC-RK-PS:05,
  author =        {A. C. C. Coolen and R. K{\"u}hn and P. Sollich},
  publisher =     {Oxford University Press},
  title =         {Theory of Neural Information Processing Systems},
  year =          {2005},
}

@book{HT-KCT-ZY:07,
  author =        {Tang, Huajin and Tan, Kay Chen and Yi, Zhang},
  publisher =     {Springer},
  title =         {Neural Networks: Computational Models and
                   Applications},
  year =          {2007},
}

@book{SH:08,
  author =        {S. Haykin},
  edition =       {3},
  publisher =     {Prentice Hall},
  title =         {Neural Networks and Learning Machines},
  year =          {2008},
  isbn =          {0131471392},
}

@book{GBE-DHT:10,
  author =        {G. Bard Ermentrout and David H. Terman},
  publisher =     {Springer},
  title =         {Mathematical Foundations of Neuroscience},
  year =          {2010},
  doi =           {10.1007/978-0-387-87708-2},
}

@book{TK:12,
  author =        {T. Kohonen},
  edition =       {3},
  publisher =     {Springer},
  title =         {Self-Organizing Maps},
  year =          {2012},
  isbn =          {9783642569272},
}

@book{WG-WMK-RN-LP:14,
  author =        {W. Gerstner and W. M. Kistler and R. Naud and
                   L. Paninski},
  publisher =     {Cambridge University Press},
  title =         {Neuronal Dynamics: From Single Neurons To Networks
                   and Models of Cognition},
  year =          {2014},
  isbn =          {9781107635197},
  url =           {https://neuronaldynamics.epfl.ch},
}

@article{AK-IS-GEH:12,
  author =        {A. Krizhevsky and I. Sutskever and G. E. Hinton},
  journal =       {Advances in Neural Information Processing Systems},
  title =         {Imagenet classification with deep convolutional
                   neural networks},
  volume =        {25},
  year =          {2012},
}

@article{CP-DBC:19,
  author =        {C. Pehlevan and D. B. Chklovskii},
  journal =       {{IEEE} Signal Processing Magazine},
  number =        {6},
  pages =         {88--96},
  title =         {Neuroscience-Inspired Online Unsupervised Learning
                   Algorithms: {Artificial} Neural Networks},
  volume =        {36},
  year =          {2019},
  abstract =      {Inventors of the original artificial neural networks
                   (ANNs) derived their inspiration from biology [1].
                   However, today, most ANNs, such as
                   backpropagation-based convolutional deeplearning
                   networks, resemble natural NNs only superficially.
                   Given that, on some tasks, such ANNs achieve human or
                   even superhuman performance, why should one care
                   about such dissimilarity with natural NNs? The
                   algorithms of natural NNs are relevant if one's goal
                   is not just to outperform humans on certain tasks but
                   to develop general-purpose artificial intelligence
                   rivaling that of a human. As contemporary ANNs are
                   far from achieving this goal and natural NNs, by
                   definition, achieve it, natural NNs must contain some
                   "secret sauce" that ANNs lack. This is why we need to
                   understand the algorithms implemented by natural
                   NNs.},
  doi =           {10.1109/msp.2019.2933846},
}

@article{AZ-etal:23,
  author =        {Zador, Anthony and Escola, Sean and Richards, Blake and
                   \"{O}lveczky, Bence and Bengio, Yoshua and
                   Boahen, Kwabena and Botvinick, Matthew and
                   Chklovskii, Dmitri and Churchland, Anne and
                   Clopath, Claudia and DiCarlo, James and
                   Ganguli, Surya and Hawkins, Jeff and
                   K\"{o}rding, Konrad and Koulakov, Alexei and
                   LeCun, Yann and Lillicrap, Timothy and
                   Marblestone, Adam and Olshausen, Bruno and
                   Pouget, Alexandre and Savin, Cristina and
                   Sejnowski, Terrence and Simoncelli, Eero and
                   Solla, Sara and Sussillo, David and
                   Tolias, Andreas S. and Tsao, Doris},
  journal =       {Nature Communications},
  number =        {1},
  title =         {Catalyzing next-generation {Artificial}
                   {Intelligence} through {NeuroAI}},
  volume =        {14},
  year =          {2023},
  doi =           {10.1038/s41467-023-37180-x},
}

@article{JGW-ES-JNT-SB:86,
  author =        {J. G. White and E. Southgate and J. N. Thomson and
                   S. Brenner},
  journal =       {Philosophical Transactions of the Royal Society of
                   London. B, Biological Sciences},
  number =        {1165},
  pages =         {1-340},
  title =         {The structure of the nervous system of the nematode
                   \emph{Caenorhabditis elegans}},
  volume =        {314},
  year =          {1986},
  abstract =      {The structure and connectivity of the nervous system
                   of the nematode Caenorhabditis elegans has been
                   deduced from reconstructions of electron micrographs
                   of serial sections. The hermaphrodite nervous system
                   has a total complement of 302 neurons, which are
                   arranged in an essentially invariant structure.
                   Neurons with similar morphologies and connectivities
                   have been grouped together into classes; there are
                   118 such classes. Neurons have simple morphologies
                   with few, if any, branches. Processes from neurons
                   run in defined positions within bundles of parallel
                   processes, synaptic connections being made en
                   passant. Process bundles are arranged longitudinally
                   and circumferentially and are often adjacent to
                   ridges of hypodermis. Neurons are generally highly
                   locally connected, making synaptic connections with
                   many of their neighbours. Muscle cells have arms that
                   run out to process bundles containing motoneuron
                   axons. Here they receive their synaptic input in
                   defined regions along the surface of the bundles,
                   where motoneuron axons reside. Most of the
                   morphologically identifiable synaptic connections in
                   a typical animal are described. These consist of
                   about 5000 chemical synapses, 2000 neuromuscular
                   junctions and 600 gap junctions.},
  doi =           {10.1098/rstb.1986.0056},
}

@article{NS-MLA-NP:20,
  author =        {N. Strisciuglio and M. Lopez-Antequera and N. Petkov},
  journal =       {Neural Computing and Applications},
  number =        {24},
  pages =         {17957-17971},
  title =         {Enhanced robustness of convolutional networks with a
                   push-pull inhibition layer},
  volume =        {32},
  year =          {2020},
  doi =           {10.1007/s00521-020-04751-8},
}

@article{GY-PEV-EKT-YLC-DSW-WRS-ALB:17,
  author =        {G. Yan and P. E. V{\'{e}}rtes and E. K. Towlson and
                   Y. L. Chew and D. S. Walker and W. R. Schafer and
                   A.-L. Barab{\'{a}}si},
  journal =       {Nature},
  number =        {7677},
  pages =         {519--523},
  title =         {Network control principles predict neuron function in
                   the {Caenorhabditis elegans} connectome},
  volume =        {550},
  year =          {2017},
  doi =           {10.1038/nature24056},
}

@article{SJC-etal:19,
  author =        {Cook, Steven J. and Jarrell, Travis A. and
                   Brittin, Christopher A. and Wang, Yi and
                   Bloniarz, Adam E. and Yakovlev, Maksim A. and
                   Nguyen, Ken C. Q. and Tang, Leo T.-H. and
                   Bayer, Emily A. and Duerr, Janet S. and
                   B\"{u}low, Hannes E. and Hobert, Oliver and
                   Hall, David H. and Emmons, Scott W.},
  journal =       {Nature},
  number =        {7763},
  pages =         {63-71},
  title =         {Whole-animal connectomes of both \emph{Caenorhabditis
                   elegans} sexes},
  volume =        {571},
  year =          {2019},
  doi =           {10.1038/s41586-019-1352-7},
}

@article{WM-etal:23,
  author =        {M. Winding and others},
  journal =       {Science},
  number =        {6636},
  title =         {The connectome of an insect brain},
  volume =        {379},
  year =          {2023},
  doi =           {10.1126/science.add9330},
}

@article{SD-etal:24,
  author =        {S. Dorkenwald and others},
  journal =       {Nature},
  number =        {8032},
  pages =         {124-138},
  title =         {Neuronal wiring diagram of an adult brain},
  volume =        {634},
  year =          {2024},
  abstract =      {Connections between neurons can be mapped by
                   acquiring and analysing electron microscopic brain
                   images. In recent years, this approach has been
                   applied to chunks of brains to reconstruct local
                   connectivity maps that are highly informative, but
                   nevertheless inadequate for understanding brain
                   function more globally. Here we present a neuronal
                   wiring diagram of a whole brain containing 5×107
                   chemical synapses between 139,255 neurons
                   reconstructed from an adult female Drosophila
                   melanogaster. The resource also incorporates
                   annotations of cell classes and types, nerves,
                   hemilineages and predictions of neurotransmitter
                   identities. Data products are available for download,
                   programmatic access and interactive browsing and have
                   been made interoperable with other fly data
                   resources. We derive a projectome—a map of
                   projections between regions—from the connectome and
                   report on tracing of synaptic pathways and the
                   analysis of information flow from inputs (sensory and
                   ascending neurons) to outputs (motor, endocrine and
                   descending neurons) across both hemispheres and
                   between the central brain and the optic lobes.
                   Tracing from a subset of photoreceptors to descending
                   motor pathways illustrates how structure can uncover
                   putative circuit mechanisms underlying sensorimotor
                   behaviours. The technologies and open ecosystem
                   reported here set the stage for future large-scale
                   connectome projects in other species.},
  doi =           {10.1038/s41586-024-07558-y},
}

@article{ALR-BEA-RER:06,
  author =        {A. L. Ridgel and B. E. Alexander and R. E. Ritzmann},
  journal =       {Journal of Comparative Physiology A},
  number =        {4},
  pages =         {385-402},
  title =         {Descending control of turning behavior in the
                   cockroach, \emph{Blaberus discoidalis}},
  volume =        {193},
  year =          {2006},
  doi =           {10.1007/s00359-006-0193-7},
}

@article{LS-etal:18,
  author =        {Schulze, Lisanne and Henninger, J\"{o}rg and
                   Kadobianskyi, Mykola and Chaigne, Thomas and
                   Faustino, Ana Isabel and Hakiy, Nahid and
                   Albadri, Shahad and Schuelke, Markus and
                   Maler, Leonard and Del Bene, Filippo and
                   Judkewitz, Benjamin},
  journal =       {Nature Methods},
  number =        {11},
  pages =         {977-983},
  title =         {Transparent \emph{Danionella translucida} as a
                   genetically tractable vertebrate brain model},
  volume =        {15},
  year =          {2018},
  doi =           {10.1038/s41592-018-0144-6},
}

@article{RB-KWC-LR:21,
  author =        {R. Britz and K. W. Conway and L. R\"{u}ber},
  journal =       {Scientific Reports},
  number =        {1},
  title =         {The emerging vertebrate model species for
                   neurophysiological studies is \emph{Danionella
                   cerebrum}, new species (Teleostei: Cyprinidae)},
  volume =        {11},
  year =          {2021},
  doi =           {10.1038/s41598-021-97600-0},
  issn =          {2045-2322},
  url =           {http://dx.doi.org/10.1038/s41598-021-97600-0},
}

@article{DCVE-etal:13,
  author =        {D. C. {Van~Essen} and and S. M. Smith and D. M. Barch and
                   T. E. J. Behrens and E. Yacoub and K. Ugurbil},
  journal =       {NeuroImage},
  pages =         {62-79},
  publisher =     {Elsevier BV},
  title =         {The {WU-Minn} {Human} {Connectome} {Project}: {An}
                   overview},
  volume =        {80},
  year =          {2013},
  doi =           {10.1016/j.neuroimage.2013.05.041},
}

@article{SWO-etal:14,
  author =        {Oh, Seung Wook and Harris, Julie A. and Ng, Lydia and
                   Winslow, Brent and Cain, Nicholas and Mihalas, Stefan and
                   Wang, Quanxin and Lau, Chris and Kuan, Leonard and
                   Henry, Alex M. and Mortrud, Marty T. and
                   Ouellette, Benjamin and Nguyen, Thuc Nghi and
                   Sorensen, Staci A. and Slaughterbeck, Clifford R. and
                   Wakeman, Wayne and Li, Yang and Feng, David and
                   Ho, Anh and Nicholas, Eric and Hirokawa, Karla E. and
                   Bohn, Phillip and Joines, Kevin M. and Peng, Hanchuan and
                   Hawrylycz, Michael J. and Phillips, John W. and
                   Hohmann, John G. and Wohnoutka, Paul and
                   Gerfen, Charles R. and Koch, Christof and
                   Bernard, Amy and Dang, Chinh and Jones, Allan R. and
                   Zeng, Hongkui},
  journal =       {Nature},
  number =        {7495},
  pages =         {207-214},
  title =         {A mesoscale connectome of the mouse brain},
  volume =        {508},
  year =          {2014},
  doi =           {10.1038/nature13186},
}

@article{JJH:1982,
  author =        {J. J. Hopfield},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {8},
  pages =         {2554--2558},
  title =         {Neural networks and physical systems with emergent
                   collective computational abilities},
  volume =        {79},
  year =          {1982},
  abstract =      {Computational properties of use of biological
                   organisms or to the construction of computers can
                   emerge as collective properties of systems having a
                   large number of simple equivalent components (or
                   neurons). The physical meaning of content-addressable
                   memory is described by an appropriate phase space
                   flow of the state of a system. A model of such a
                   system is given, based on aspects of neurobiology but
                   readily adapted to integrated circuits. The
                   collective properties of this model produce a
                   content-addressable memory which correctly yields an
                   entire memory from any subpart of sufficient size.
                   The algorithm for the time evolution of the state of
                   the system is based on asynchronous parallel
                   processing. Additional emergent collective properties
                   include some capacity for generalization, familiarity
                   recognition, categorization, error correction, and
                   time sequence retention. The collective properties
                   are only weakly sensitive to details of the modeling
                   or the failure of individual devices.},
  doi =           {10.1073/pnas.79.8.2554},
}

@article{JJH:84,
  author =        {J. J. Hopfield},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {10},
  pages =         {3088-3092},
  title =         {Neurons with graded response have collective
                   computational properties like those of two-state
                   neurons},
  volume =        {81},
  year =          {1984},
  abstract =      {A model for a large network of "neurons" with a
                   graded response (or sigmoid input-output relation) is
                   studied. This deterministic system has collective
                   properties in very close correspondence with the
                   earlier stochastic model based on McCulloch - Pitts
                   neurons. The content- addressable memory and other
                   emergent collective properties of the original model
                   also are present in the graded response model. The
                   idea that such collective properties are used in
                   biological systems is given added credence by the
                   continued presence of such properties for more nearly
                   biological "neurons." Collective analog electrical
                   circuits of the kind described will certainly
                   function. The collective states of the two models
                   have a simple correspondence. The original model will
                   continue to be useful for simulations, because its
                   connection to graded response systems is established.
                   Equations that include the effect of action
                   potentials in the graded response system are also
                   developed.},
  doi =           {10.1073/pnas.81.10.3088},
}

@article{EO:82,
  author =        {E. Oja},
  journal =       {Journal of Mathematical Biology},
  number =        {3},
  pages =         {267--273},
  title =         {Simplified neuron model as a principal component
                   analyzer},
  volume =        {15},
  year =          {1982},
  doi =           {10.1007/bf00275687},
}

@book{JW-YM:22,
  author =        {J. Wright and Y. Ma},
  publisher =     {Cambridge University Press},
  title =         {High-Dimensional Data Analysis with Low-Dimensional
                   Models: Principles, Computation, and Applications},
  year =          {2022},
}

@article{HRW-JDC:72,
  author =        {H. R. Wilson and J. D. Cowan},
  journal =       {Biophysical Journal},
  number =        {1},
  pages =         {1-24},
  title =         {Excitatory and Inhibitory Interactions in Localized
                   Populations of Model Neurons},
  volume =        {12},
  year =          {1972},
  abstract =      {Coupled nonlinear differential equations are derived
                   for the dynamics of spatially localized populations
                   containing both excitatory and inhibitory model
                   neurons. Phase plane methods and numerical solutions
                   are then used to investigate population responses to
                   various types of stimuli. The results obtained show
                   simple and multiple hysteresis phenomena and limit
                   cycle activity. The latter is particularly
                   interesting since the frequency of the limit cycle
                   oscillation is found to be a monotonic function of
                   stimulus intensity. Finally, it is proved that the
                   existence of limit cycle dynamics in response to one
                   class of stimuli implies the existence of multiple
                   stable states and hysteresis in response to a
                   different class of stimuli. The relation between
                   these findings and a number of experiments is
                   discussed.},
  doi =           {10.1016/s0006-3495(72)86068-5},
}

@article{KDM-FF:12,
  author =        {K. D. Miller and F. Fumarola},
  journal =       {Neural Computation},
  number =        {1},
  pages =         {25-31},
  title =         {Mathematical Equivalence of Two Common Forms of
                   Firing Rate Models of Neural Networks},
  volume =        {24},
  year =          {2012},
  doi =           {10.1162/NECO_a_00221},
}

@book{DOH:49,
  author =        {Donald O. Hebb},
  publisher =     {John Wiley \& Sons},
  title =         {The Organization of Behavior: {A} Neuropsychological
                   Theory},
  year =          {1949},
  doi =           {10.1002/sce.37303405110},
}

@book{SRC:1904,
  author =        {Santiago {Ramón y Cajal}},
  publisher =     {Imprenta y Librería de N.~Moya},
  title =         {Textura Del Sistema Nervioso Del Hombre y de Los
                   Vertebrados},
  year =          {1904},
}

@article{WG-WK:02,
  author =        {W. Gerstner and W. Kistler},
  journal =       {Biological Cybernetics},
  pages =         {404-15},
  title =         {Mathematical Formulations of {Hebbian} Learning},
  volume =        {87},
  year =          {2003},
  abstract =      {Several formulations of correlation-based Hebbian
                   learning are reviewed. On the presynaptic side,
                   activity is described either by a firing rate or by
                   presynaptic spike arrival. The state of the
                   postsynaptic neuron can be described by its membrane
                   potential, its firing rate, or the timing of
                   backpropagating action potentials (BPAPs). It is
                   shown that all of the above formulations can be
                   derived from the point of view of an expansion. In
                   the absence of BPAPs, it is natural to correlate
                   presynaptic spikes with the postsynaptic membrane
                   potential. Time windows of spike-time-dependent
                   plasticity arise naturally if the timing of
                   postsynaptic spikes is available at the site of the
                   synapse, as is the case in the presence of BPAPs.
                   With an appropriate choice of parameters, Hebbian
                   synaptic plasticity has intrinsic normalization
                   properties that stabilizes postsynaptic firing rates
                   and leads to subtractive weight normalization.},
  doi =           {10.1007/s00422-002-0353-y},
}

@article{MAC-SG:83,
  author =        {Cohen, M. A. and Grossberg, S.},
  journal =       {IEEE Transactions on Systems, Man and Cybernetics},
  number =        {5},
  pages =         {815--826},
  title =         {Absolute stability of global pattern formation and
                   parallel memory storage by competitive neural
                   networks},
  volume =        {13},
  year =          {1983},
  doi =           {10.1109/TSMC.1983.6313075},
}

@inproceedings{HR-eal:20,
  author =        {H. Ramsauer and B. Sch{\"a}fl and J. Lehner and
                   P. Seidl and M. Widrich and L. Gruber and
                   M. Holzleitner and T. Adler and D. Kreil and
                   M. K. Kopp and G. Klambauer and J. Brandstetter and
                   S. Hochreiter},
  booktitle =     {International Conference on Learning Representations},
  title =         {Hopfield Networks is All You Need},
  year =          {2021},
  url =           {https://openreview.net/forum?id=tL89RnzIiCd},
}

@inproceedings{DK-JJH:21,
  author =        {D. Krotov and J. J. Hopfield},
  booktitle =     {International Conference on Learning Representations},
  title =         {Large Associative Memory Problem in Neurobiology and
                   Machine Learning},
  year =          {2021},
  url =           {https://openreview.net/forum?id=X4y_10OX-hX},
}

@article{LEG-FG-BT-AA-AYT:21,
  author =        {{El~Ghaoui}, L. and Gu, F. and Travacca, B. and
                   Askari, A. and Tsai, A.},
  journal =       {SIAM Journal on Mathematics of Data Science},
  number =        {3},
  pages =         {930-958},
  title =         {Implicit Deep Learning},
  volume =        {3},
  year =          {2021},
  abstract =      {Implicit deep learning prediction rules generalize
                   the recursive rules of feedforward neural networks.
                   Such rules are based on the solution of a fixed-point
                   equation involving a single vector of hidden
                   features, which is thus only implicitly defined. The
                   implicit framework greatly simplifies the notation of
                   deep learning, and opens up many new possibilities in
                   terms of novel architectures and algorithms,
                   robustness analysis and design, interpretability,
                   sparsity, and network architecture optimization.},
  doi =           {10.1137/20M1358517},
}

@article{PLC-JCP:21,
  author =        {P. L. {Combettes} and J.-C. {Pesquet}},
  journal =       {IEEE Transactions on Signal Processing},
  pages =         {3878-3905},
  title =         {Fixed Point Strategies in Data Science},
  volume =        {69},
  year =          {2021},
  abstract =      {The goal of this paper is to promote the use of fixed
                   point strategies in data science by showing that they
                   provide a simplifying and unifying framework to
                   model, analyze, and solve a great variety of
                   problems. They are seen to constitute a natural
                   environment to explain the behavior of advanced
                   convex optimization methods as well as of recent
                   nonlinear methods in data science which are
                   formulated in terms of paradigms that go beyond
                   minimization concepts and involve constructs such as
                   Nash equilibria or monotone inclusions. We review the
                   pertinent tools of fixed point theory and describe
                   the main state-of-the-art iterative algorithms for
                   provably convergent fixed point construction. We also
                   incorporate additional ingredients such as
                   stochasticity, block-implementations, and
                   non-Euclidean metrics, which provide further
                   enhancements. Applications to signal and image
                   processing, machine learning, statistics, neural
                   networks, and inverse problems are discussed.},
  doi =           {10.1109/TSP.2021.3069677},
}

@inproceedings{SJ-AD-AVP-FB:21f,
  author =        {S. Jafarpour and A. Davydov and A. V. Proskurnikov and
                   F. Bullo},
  booktitle =     {Advances in Neural Information Processing Systems},
  month =         dec,
  title =         {Robust Implicit Networks via Non-{Euclidean}
                   Contractions},
  year =          {2021},
  abstract =      {Implicit neural networks, a.k.a., deep equilibrium
                   networks, are a class of implicit-depth learning
                   models where function evaluation is performed by
                   solving a fixed point equation. They generalize
                   classic feedforward models and are equivalent to
                   infinite-depth weight-tied feedforward networks.
                   While implicit models show improved accuracy and
                   significant reduction in memory consumption, they can
                   suffer from ill-posedness and convergence
                   instability. This paper provides a new framework to
                   design well-posed and robust implicit neural networks
                   based upon contraction theory for the non-Euclidean
                   norm $\ell_\infty$. Our framework includes (i) a
                   novel condition for well-posedness based on one-sided
                   Lipschitz constants, (ii) an average iteration for
                   computing fixed-points, and (iii) explicit estimates
                   on input-output Lipschitz constants. Additionally, we
                   design a training problem with the well-posedness
                   condition and the average iteration as constraints
                   and, to achieve robust models, with the input-output
                   Lipschitz constant as a regularizer. Our
                   $\ell_\infty$ well-posedness condition leads to a
                   larger polytopic training search space than existing
                   conditions and our average iteration enjoys
                   accelerated convergence. Finally, we perform several
                   numerical experiments for function estimation and
                   digit classification through the MNIST data set. Our
                   numerical results demonstrate improved accuracy and
                   robustness of the implicit models with smaller
                   input-output Lipschitz bounds.},
  doi =           {10.48550/arXiv.2106.03194},
}

@article{BWM:93,
  author =        {Mel, B. W.},
  journal =       {Journal of Neurophysiology},
  number =        {3},
  pages =         {1086-1101},
  title =         {Synaptic integration in an excitable dendritic tree},
  volume =        {70},
  year =          {1993},
  abstract =      {this work provides further support for a novel
                   principle of dendritic information processing that
                   could underlie a capacity for nonlinear pattern
                   discrimination and / or sensory pro- cessing within
                   the dendritic trees of individual nerve cells. It was
                   previously demonstrated that when excitatory synap-
                   tic input to a pyramidal cell is dominated by
                   voltage-dependent N-methyl-o-aspartate ( NMDA )-type
                   channels, the cell responds more strongly when
                   synaptic drive is concentrated within several
                   dendritic regions than when it is delivered diffusely
                   across the dendritic arbor. This effect, called
                   dendritic “cluster sensitivity,” persisted under
                   wide-ranging parameter variations and directly
                   implicated the spatial ordering of afferent synaptic
                   connections onto the dendritic tree as an important
                   determinant of neuronal response selectivity. 3. In
                   this work, the sensitivity of neocortical dendrites
                   to spa- tially clustered synaptic drive has been
                   further studied with fast sodium and slow calcium
                   spiking mechanisms present in the den- dritic
                   membrane. Several spatial distributions of the
                   dendritic spiking mechanisms were tested with and
                   without NMDA syn- apses. Results of numerous
                   simulations reveal that dendritic clus- ter
                   sensitivity is a highly robust phenomenon in
                   dendrites contain- ing a sufficiency of excitatory
                   membrane mechanisms and is only weakly dependent on
                   their detailed spatial distribution, peak con-
                   ductances, or kinetics. Factors that either work
                   against or make irrelevant the dendritic},
  doi =           {10.1152/jn.1993.70.3.1086},
}

@article{ES-TJS:01,
  author =        {Salinas, Emilio and Sejnowski, Terrence J.},
  journal =       {Nature Reviews Neuroscience},
  number =        {8},
  pages =         {539-550},
  title =         {Correlated neuronal activity and the flow of neural
                   information},
  volume =        {2},
  year =          {2001},
  abstract =      {For years we have known that cortical neurons
                   collectively have synchronous or oscillatory patterns
                   of activity, the frequencies and temporal dynamics of
                   which are associated with distinct behavioural
                   states. Although the function of these oscillations
                   has remained obscure, recent experimental and
                   theoretical results indicate that correlated
                   fluctuations might be important for cortical
                   processes, such as attention, that control the flow
                   of information in the brain.},
  doi =           {10.1038/35086012},
}

@article{ML-MH:05,
  author =        {M. London and M. H\"{a}usser},
  journal =       {Annual Review of Neuroscience},
  number =        {1},
  pages =         {503-532},
  title =         {Dendritic Computation},
  volume =        {28},
  year =          {2005},
  abstract =      {Abstract One of the central questions in neuroscience
                   is how particular tasks, or computations, are
                   implemented by neural networks to generate behavior.
                   The prevailing view has been that information
                   processing in neural networks results primarily from
                   the properties of synapses and the connectivity of
                   neurons within the network, with the intrinsic
                   excitability of single neurons playing a lesser role.
                   As a consequence, the contribution of single neurons
                   to computation in the brain has long been
                   underestimated. Here we review recent work showing
                   that neuronal dendrites exhibit a range of linear and
                   nonlinear mechanisms that allow them to implement
                   elementary computations. We discuss why these
                   dendritic properties may be essential for the
                   computations performed by the neuron and the network
                   and provide theoretical and experimental examples to
                   support this view.},
  doi =           {10.1146/annurev.neuro.28.061604.135703},
}

@article{PP-AP:20,
  author =        {Poirazi, Panayiota and Papoutsi, Athanasia},
  journal =       {Nature Reviews Neuroscience},
  number =        {6},
  pages =         {303-321},
  title =         {Illuminating dendritic function with computational
                   models},
  volume =        {21},
  year =          {2020},
  abstract =      {Dendrites have always fascinated researchers: from
                   the artistic drawings by Ramon y Cajal to the
                   beautiful recordings of today, neuroscientists have
                   been striving to unravel the mysteries of these
                   structures. Theoretical work in the 1960s predicted
                   important dendritic effects on neuronal processing,
                   establishing computational modelling as a powerful
                   technique for their investigation. Since then,
                   modelling of dendrites has been instrumental in
                   driving neuroscience research in a targeted manner,
                   providing experimentally testable predictions that
                   range from the subcellular level to the systems
                   level, and their relevance extends to fields beyond
                   neuroscience, such as machine learning and artificial
                   intelligence. Validation of modelling predictions
                   often requires — and drives — new technological
                   advances, thus closing the loop with theory-driven
                   experimentation that moves the field forward. This
                   Review features the most important, to our
                   understanding, contributions of modelling of
                   dendritic computations, including those pending
                   experimental verification, and highlights studies of
                   successful interactions between the modelling and
                   experimental neuroscience communities.},
  doi =           {10.1038/s41583-020-0301-7},
}

@article{SG:88,
  author =        {S. Grossberg},
  journal =       {Neural Networks},
  number =        {1},
  pages =         {17--61},
  title =         {Nonlinear neural networks: {Principles}, mechanisms,
                   and architectures},
  volume =        {1},
  year =          {1988},
  abstract =      {An historical discussion is provided of the
                   intellectual trends that caused nineteenth century
                   interdisciplinary studies of physics and
                   psychobiology by leading scientists such as
                   Helmholtz, Maxwell, and Mach to splinter into
                   separate twentieth-century scientific movements. The
                   nonlinear, nonstationary, and nonlocal nature of
                   behavioral and brain data are emphasized. Three
                   sources of contemporary neural network research-the
                   binary, linear, and continuous-nonlinear models-are
                   noted. The remainder of the article describes results
                   about continuous-nonlinear models: Many models of
                   content-addressable memory are shown to be special
                   cases of the Cohen-Grossberg model and global
                   Liapunov function, including the additive,
                   brain-state-in-a-box, McCulloch-Pitts, Boltzmann
                   machine, Hartline-Ratliff-Miller, shunting, masking
                   field, bidirectional associative memory,
                   Volterra-Lotka, Gilpin-Ayala, and Eigen-Schuster
                   models. A Liapunov functional method is described for
                   proving global limit or oscillation theorems for
                   nonlinear competitive systems when their decision
                   schemes are globally consistent or inconsistent,
                   respectively. The former case is illustrated by a
                   model of a globally stable economic market, and the
                   latter case is illustrated by a model of the voting
                   paradox. Key properties of shunting competitive
                   feedback networks are summarized, including the role
                   of sigmoid signalling, automatic gain control,
                   competitive choice and quantization, tunable
                   filtering, total activity normalization, and noise
                   suppression in pattern transformation and memory
                   storage applications. Connections to models of
                   competitive learning, vector quantization, and
                   categorical perception are noted. Adaptive resonance
                   theory (ART) models for self-stabilizing adaptive
                   pattern recognition in response to complex real-time
                   nonstationary input environments are compared with
                   off-line models such as autoassociators, the
                   Boltzmann machine, and back propagation. Special
                   attention is paid to the stability and capacity of
                   these models, and to the role of top-down
                   expectations and attentional processing in the active
                   regulation of both learning and fast information
                   processing. Models whose performance and learning are
                   regulated by internal gating and matching signals, or
                   by external environmentally generated error signals,
                   are contrasted with models whose learning is
                   regulated by external teacher signals that have no
                   analog in natural real-time environments. Examples
                   from sensory-motor control of adaptive vector
                   encoders, adaptive coordinate transformations,
                   adaptive gain control by visual error signals, and
                   automatic generation of synchronous multijoint
                   movement trajectories illustrate the former model
                   types. Internal matching processes are shown capable
                   of discovering several different types of invariant
                   environmental properties. These include ART
                   mechanisms which discover recognition invariants,
                   adaptive vector encoder mechanisms which discover
                   movement invariants, and autoreceptive associative
                   mechanisms which discover invariants of
                   self-regulating target position maps.},
  doi =           {10.1016/0893-6080(88)90021-4},
}

@incollection{JJH:07,
  author =        {J. J. Hopfield},
  booktitle =     {Scholarpedia},
  publisher =     {(Online)},
  title =         {Hopfield network},
  year =          {2007},
  doi =           {10.4249/scholarpedia.1977},
}

@incollection{SG:13,
  author =        {S. Grossberg},
  booktitle =     {Scholarpedia},
  publisher =     {(Online)},
  title =         {Recurrent Neural Networks},
  year =          {2013},
  doi =           {10.4249/scholarpedia.1888},
}

@inproceedings{MS-JO:22,
  author =        {M. Snow and J. Orchard},
  booktitle =     {Proceedings of the Annual Meeting of the Cognitive
                   Science Society},
  title =         {Biological softmax: {Demonstrated} in modern
                   {Hopfield} networks},
  volume =        {44},
  year =          {2022},
  url =           {https://escholarship.org/uc/item/3jd1t2hr},
}

@article{BG-LP:17,
  author =        {B. Gao and L. Pavel},
  journal =       {arXiv preprint arXiv:1704.00805},
  title =         {On the properties of the softmax function with
                   application in game theory and reinforcement
                   learning},
  year =          {2017},
  doi =           {10.48550/arXiv.1704.00805},
}

@article{MN:1942,
  author =        {M. Nagumo},
  journal =       {Proceedings of the Physico-Mathematical Society of
                   Japan. 3rd Series},
  pages =         {551-559},
  title =         {{\"Uber die Lage der Integralkurven gew\"ohnlicher
                   Differentialgleichungen}},
  volume =        {24},
  year =          {1942},
  doi =           {10.11429/ppmsj1919.24.0_551},
}

@article{FB:99,
  author =        {F. Blanchini},
  journal =       {Automatica},
  number =        {11},
  pages =         {1747-1767},
  title =         {Set invariance in control},
  volume =        {35},
  year =          {1999},
  doi =           {10.1016/S0005-1098(99)00113-2},
}

@book{FB-SM:15,
  author =        {F. Blanchini and S. Miani},
  publisher =     {Springer},
  title =         {Set-Theoretic Methods in Control},
  year =          {2015},
  isbn =          {9783319179322},
}

@article{MF-AT:95,
  author =        {M. Forti and A. Tesi},
  journal =       {IEEE Transactions on Circuits and Systems I:
                   Fundamental Theory and Applications},
  number =        {7},
  pages =         {354-366},
  title =         {New conditions for global stability of neural
                   networks with application to linear and quadratic
                   programming problems},
  volume =        {42},
  year =          {1995},
  abstract =      {In this paper, we present new conditions ensuring
                   existence, uniqueness, and Global Asymptotic
                   Stability (GAS) of the equilibrium point for a large
                   class of neural networks. The results are applicable
                   to both symmetric and nonsymmetric interconnection
                   matrices and allow for the consideration of all
                   continuous nondecreasing neuron activation functions.
                   Such functions may be unbounded (but not necessarily
                   surjective), may have infinite intervals with zero
                   slope as in a piece-wise-linear model, or both. The
                   conditions on GAS rely on the concept of Lyapunov
                   Diagonally Stable (or Lyapunov Diagonally
                   Semi-Stable) matrices and are proved by employing a
                   class of Lyapunov functions of the generalized
                   Lur'e-Postnikov type. Several classes of
                   interconnection matrices of applicative interest are
                   shown to satisfy our conditions for GAS. In
                   particular, the results are applied to analyze GAS
                   for the class of neural circuits introduced for
                   solving linear and quadratic programming problems. In
                   this application, the principal result here obtained
                   is that these networks are GAS also when the
                   constraint amplifiers are dynamical, as it happens in
                   any practical implementation.<>},
  doi =           {10.1109/81.401145},
}

@book{EK-AB:00,
  author =        {E. Kaszkurewicz and A. Bhaya},
  publisher =     {Springer},
  title =         {Matrix Diagonal Stability in Systems and Computation},
  year =          {2000},
  isbn =          {978-0-8176-4088-0},
}

@book{SB-LV:04,
  author =        {S. Boyd and L. Vandenberghe},
  publisher =     {Cambridge University Press},
  title =         {Convex Optimization},
  year =          {2004},
  isbn =          {0521833787},
}

@misc{MG-SB:11-cvx,
  author =        {M. Grant and S. Boyd},
  month =         mar,
  title =         {{CVX}: Matlab Software for Disciplined Convex
                   Programming, version 2.1},
  year =          {2014},
  url =           {http://cvxr.com/cvx},
}

@article{EK-AB:93,
  author =        {E. Kaszkurewicz and A. Bhaya},
  journal =       {{SIAM} Journal on Matrix Analysis and Applications},
  number =        {2},
  pages =         {508--520},
  title =         {Robust Stability and Diagonal {Liapunov} Functions},
  volume =        {14},
  year =          {1993},
  doi =           {10.1137/0614036},
}

@book{SB-LEG-EF-VB:94,
  author =        {S. Boyd and L. {El~Ghaoui} and E. Feron and
                   V. Balakrishnan},
  publisher =     {SIAM},
  title =         {Linear Matrix Inequalities in System and Control
                   Theory},
  year =          {1994},
  isbn =          {089871334X},
}

@article{GWC:78,
  author =        {G. W. Cross},
  journal =       {Linear Algebra and its Applications},
  number =        {3},
  pages =         {253-263},
  title =         {Three types of matrix stability},
  volume =        {20},
  year =          {1978},
  doi =           {http://dx.doi.org/10.1016/0024-3795(78)90021-6},
}

@article{DH:92,
  author =        {D. Hershkowitz},
  journal =       {Linear Algebra and its Applications},
  pages =         {161--186},
  title =         {Recent directions in matrix stability},
  volume =        {171},
  year =          {1992},
  doi =           {10.1016/0024-3795(92)90257-b},
}

@article{MArcak:11,
  author =        {M. Arcak},
  journal =       {IEEE Transactions on Automatic Control},
  number =        {12},
  pages =         {2766--2777},
  title =         {Diagonal stability on cactus graphs and application
                   to network stability analysis},
  volume =        {56},
  year =          {2011},
  doi =           {10.1109/TAC.2011.2125130},
}

@article{DJA-HG-HS:87,
  author =        {Amit, D. J. and Gutfreund, H. and Sompolinsky, H.},
  journal =       {Annals of Physics},
  number =        {1},
  pages =         {30-67},
  title =         {Statistical mechanics of neural networks near
                   saturation},
  volume =        {173},
  year =          {1987},
  url =           {10.1016/0003-4916(87)90092-3},
}

@article{AC-DJA-HG:86,
  author =        {Crisanti, A. and Amit, D. J. and and Gutfreund, H.},
  journal =       {Europhysics Letters},
  number =        {4},
  pages =         {337-341},
  title =         {Saturation Level of the {Hopfield} Model for Neural
                   Network},
  volume =        {2},
  year =          {1986},
  doi =           {10.1209/0295-5075/2/4/012},
}

@article{AT-DJA:88,
  author =        {Treves, A. and Amit, D. J.},
  journal =       {Journal of Physics A: Mathematical and General},
  number =        {14},
  pages =         {3155-3169},
  title =         {Metastable states in asymmetrically diluted
                   {Hopfield} networks},
  volume =        {21},
  year =          {1988},
  doi =           {10.1088/0305-4470/21/14/016},
}

@article{ANM-JAF-WP:89,
  author =        {A. N. {Michel} and J. A. {Farrell} and W. {Porod}},
  journal =       {IEEE Transactions on Circuits and Systems},
  number =        {2},
  pages =         {229-243},
  title =         {Qualitative analysis of neural networks},
  volume =        {36},
  year =          {1989},
  doi =           {10.1109/31.20200},
}

@article{SB-GB-FB-SZ:23k,
  author =        {S. Betteti and G. Baggio and F. Bullo and
                   S. Zampieri},
  journal =       {Science Advances},
  number =        {17},
  title =         {Stimulus-Driven Dynamics for Robust Memory Retrieval
                   in {Hopfield} Networks},
  volume =        {11},
  year =          {2025},
  abstract =      {The Hopfield model provides a mathematical framework
                   for understanding the mechanisms of memory storage
                   and retrieval in the human brain. This model has
                   inspired decades of research on learning and
                   retrieval dynamics, capacity estimates, and
                   sequential transitions among memories. Notably, the
                   role of external inputs has been largely
                   underexplored, from their effects on neural dynamics
                   to how they facilitate effective memory retrieval. To
                   bridge this gap, we propose a dynamical system
                   framework in which the external input directly
                   influences the neural synapses and shapes the energy
                   landscape of the Hopfield model. This
                   plasticity-based mechanism provides a clear energetic
                   interpretation of the memory retrieval process and
                   proves effective at correctly classifying mixed
                   inputs. Furthermore, we integrate this model within
                   the framework of modern Hopfield architectures to
                   elucidate how current and past information are
                   combined during the retrieval process. Finally, we
                   embed both the classic and the proposed model in an
                   environment disrupted by noise and compare their
                   robustness during memory retrieval.},
  doi =           {10.1126/sciadv.adu6991},
}

@article{EK-AB:94,
  author =        {Kaszkurewicz, E. and Bhaya, A.},
  journal =       {IEEE Transactions on Circuits and Systems I:
                   Fundamental Theory and Applications},
  number =        {2},
  pages =         {171-174},
  title =         {On a class of globally stable neural circuits},
  volume =        {41},
  year =          {1994},
  abstract =      {The authors show that diagonal stability of the
                   interconnection matrix leads to a simple proof of the
                   existence, uniqueness, and global asymptotic
                   stability of the equilibrium of a Hopfield-Tank
                   neural circuit, without making some common
                   restrictive assumptions used in earlier results. It
                   is also shown that the same condition guarantees
                   structural stability, which ensures the desirable
                   property of persistence of global asymptotic
                   stability under general C/sup 1/
                   perturbations.&lt;<ETX>&gt;</ETX>},
  doi =           {10.1109/81.269055},
}

@article{MF-SM-MM:94,
  author =        {Forti, M. and Manetti, S. and Marini, M.},
  journal =       {IEEE Transactions on Circuits and Systems I:
                   Fundamental Theory and Applications},
  number =        {7},
  pages =         {491-494},
  title =         {Necessary and sufficient condition for absolute
                   stability of neural networks},
  volume =        {41},
  year =          {1994},
  abstract =      {The main result in this paper is that for a neural
                   circuit of the Hopfield type with a symmetric
                   connection matrix T, the negative semidefiniteness of
                   T is a necessary and sufficient condition for
                   Absolute Stability. The most significant theoretical
                   implication is that the class of neural circuits with
                   a negative semidefinite T is the largest class of
                   circuits that can be employed for embedding and
                   solving optimization problems without the risk of
                   spurious responses.},
  doi =           {10.1109/81.298364},
}

@article{ZY-PAH-AWCF:99,
  author =        {Y. Zhang and P. A. Heng and A. W. C. Fu},
  journal =       {IEEE Transactions on Neural Networks},
  number =        {6},
  pages =         {1487-1493},
  title =         {Estimate of exponential convergence rate and
                   exponential stability for neural networks},
  volume =        {10},
  year =          {1999},
  abstract =      {Estimates of exponential convergence rate and
                   exponential stability are studied for a class of
                   neural networks which includes Hopfield neural
                   networks and cellular neural networks. Both local and
                   global exponential convergence are discussed.
                   Theorems for estimation of the exponential
                   convergence rate are established and the bounds on
                   the rate of convergence are given. The domains of
                   attraction in the case of local exponential
                   convergence are obtained. Simple conditions are
                   presented for checking exponential stability of the
                   neural networks.},
  doi =           {10.1109/72.809094},
}

@article{TC-WL-SIA:02,
  author =        {T. Chen and W. Lu and S.-I. Amari},
  journal =       {Neural Computation},
  number =        {12},
  pages =         {2947-2957},
  title =         {Global Convergence Rate of Recurrently Connected
                   Neural Networks},
  volume =        {14},
  year =          {2002},
  abstract =      {We discuss recurrently connected neural networks,
                   investigating their global exponential stability
                   (GES). Some sufficient conditions for a class of
                   recurrent neural networks belonging to GES are given.
                   Sharp convergence rate is given too.},
  doi =           {10.1162/089976602760805359},
}

@article{YF-TGK:96,
  author =        {Y. Fang and T. G. Kincaid},
  journal =       {IEEE Transactions on Neural Networks},
  number =        {4},
  pages =         {996-1006},
  title =         {Stability analysis of dynamical neural networks},
  volume =        {7},
  year =          {1996},
  abstract =      {In this paper, we use the matrix measure technique to
                   study the stability of dynamical neural networks.
                   Testable conditions for global exponential stability
                   of nonlinear dynamical systems and dynamical neural
                   networks are given. It shows how a few well-known
                   results can be unified and generalized in a
                   straightforward way. Local exponential stability of a
                   class of dynamical neural networks is also studied;
                   we point out that the local exponential stability of
                   any equilibrium point of dynamical neural networks is
                   equivalent to the stability of the linearized system
                   around that equilibrium point. From this, some
                   well-known and new sufficient conditions for local
                   exponential stability of neural networks are
                   obtained.},
  doi =           {10.1109/72.508941},
}

@article{SA:02,
  author =        {Arik, S.},
  journal =       {IEEE Transactions on Circuits and Systems I:
                   Fundamental Theory and Applications},
  number =        {4},
  pages =         {502-504},
  title =         {A note on the global stability of dynamical neural
                   networks},
  volume =        {49},
  year =          {2002},
  abstract =      {It is shown that the additive diagonal stability
                   condition on the interconnection matrix of a neural
                   network, together with the assumption that the
                   activation functions are nondecreasing, guarantees
                   the uniqueness of the equilibrium point. This
                   condition, under the same assumption on the
                   activation functions, is also shown to imply the
                   local attractivity and local asymptotic stability of
                   the equilibrium point, thus ensuring the global
                   asymptotic stability (GAS) of the equilibrium point.
                   The result obtained generalizes the previous results
                   derived in the literature.},
  doi =           {10.1109/81.995665},
}

@article{HQ-JP-ZBX:01,
  author =        {H. Qiao and J. Peng and Z.-B. Xu},
  journal =       {IEEE Transactions on Neural Networks},
  number =        {2},
  pages =         {360-370},
  title =         {Nonlinear measures: {A} new approach to exponential
                   stability analysis for {Hopfield}-type neural
                   networks},
  volume =        {12},
  year =          {2001},
  abstract =      {In this paper, a new concept called nonlinear measure
                   is introduced to quantify stability of nonlinear
                   systems in the way similar to the matrix measure for
                   stability of linear systems. Based on the new
                   concept, a novel approach for stability analysis of
                   neural networks is developed. With this approach, a
                   series of new sufficient conditions for global and
                   local exponential stability of Hopfield type neural
                   networks is presented, which generalizes those
                   existing results. By means of the introduced
                   nonlinear measure, the exponential convergence rate
                   of the neural networks to stable equilibrium point is
                   estimated, and, for local stability, the attraction
                   region of the stable equilibrium point is
                   characterized. The developed approach can be
                   generalized to stability analysis of other general
                   nonlinear systems.},
  doi =           {10.1109/72.914530},
}

@article{HZ-ZW-DL:14,
  author =        {H. {Zhang} and Z. {Wang} and D. {Liu}},
  journal =       {IEEE Transactions on Neural Networks and Learning
                   Systems},
  number =        {7},
  pages =         {1229-1262},
  title =         {A Comprehensive Review of Stability Analysis of
                   Continuous-Time Recurrent Neural Networks},
  volume =        {25},
  year =          {2014},
  doi =           {10.1109/TNNLS.2014.2317880},
}

@book{AGK:91,
  author =        {A. G. Khovanski{\u\i}},
  publisher =     {American Mathematical Society},
  title =         {Fewnomials},
  year =          {1991},
  isbn =          {978-1-4704-4500-3},
}

@article{SKP:69,
  author =        {S. K. Persidskii},
  journal =       {Avtomatika i Telemekhanika},
  note =          {(Automation and Remote Control, 1970, 30:12,
                   1889-1895)},
  pages =         {5--11},
  title =         {Concerning problem of absolute stability},
  volume =        {12},
  year =          {1969},
}

@article{AD-AVP-FB:22q,
  author =        {A. Davydov and A. V. Proskurnikov and F. Bullo},
  journal =       {IEEE Transactions on Automatic Control},
  number =        {1},
  pages =         {235-250},
  title =         {{Non-Euclidean} Contraction Analysis of
                   Continuous-Time Neural Networks},
  volume =        {70},
  year =          {2025},
  abstract =      {Critical questions in dynamical neuroscience and
                   machine learning are related to the study of
                   continuous-time neural networks and their stability,
                   robustness, and computational efficiency. These
                   properties can be simultaneously established via a
                   contraction analysis. This paper develops a
                   comprehensive non-Euclidean contraction theory for
                   continuous-time neural networks. Specifically, we
                   provide novel sufficient conditions for the
                   contractivity of general classes of continuous-time
                   neural networks including Hopfield, firing rate,
                   Persidskii, Lur'e, and other neural networks with
                   respect to the non-Euclidean $\ell_1/\ell_\infty$
                   norms. These sufficient conditions are based upon
                   linear programming or, in some special cases,
                   establishing the Hurwitzness of a particular Metzler
                   matrix. To prove these sufficient conditions, we
                   develop novel results on non-Euclidean logarithmic
                   norms and a novel necessary and sufficient condition
                   for contractivity of systems with locally Lipschitz
                   dynamics. For each model, we apply our theoretical
                   results to compute the optimal contraction rate and
                   corresponding weighted non-Euclidean norm with
                   respect to which the neural network is contracting.},
  doi =           {10.1109/TAC.2024.3422217},
}

@article{VC-AG-AD-GR-FB:23c,
  author =        {V. Centorrino and A. Gokhale and A. Davydov and
                   G. Russo and F. Bullo},
  journal =       {IEEE Control Systems Letters},
  pages =         {1724-1729},
  title =         {Euclidean Contractivity of Neural Networks with
                   Symmetric Weights},
  volume =        {7},
  year =          {2023},
  abstract =      {This paper investigates stability conditions of
                   continuous-time Hopfield and firing-rate neural
                   networks by leveraging contraction theory. First, we
                   present a number of useful general algebraic results
                   on matrix polytopes and products of symmetric
                   matrices. Then, we give sufficient conditions for
                   strong and weak Euclidean contractivity, i.e.,
                   contractivity with respect to the $\ell_2$ norm, of
                   both models with symmetric weights and (possibly)
                   non-smooth activation functions. Our contraction
                   analysis leads to contraction rates which are
                   log-optimal in almost all symmetric synaptic
                   matrices. Finally, we use our results to propose a
                   firing-rate neural network model to solve a quadratic
                   optimization problem with box constraints.},
  doi =           {10.1109/LCSYS.2023.3278250},
}

@article{MR-RW-IRM:20,
  author =        {M. Revay and R. Wang and I. R. Manchester},
  journal =       {arXiv preprint arXiv:2010.01732},
  title =         {Lipschitz Bounded Equilibrium Networks},
  year =          {2020},
  doi =           {10.48550/arXiv.2010.01732},
}

@inproceedings{LK-ME-JJES:22,
  author =        {L. Kozachkov and M. Ennis and J.-J. E. Slotine},
  booktitle =     {Advances in Neural Information Processing Systems},
  month =         dec,
  title =         {{RNNs} of {RNNs}: {Recursive} Construction of Stable
                   Assemblies of Recurrent Neural Networks},
  year =          {2022},
  abstract =      {Recurrent neural networks (RNNs) are widely used
                   throughout neuroscience as models of local neural
                   activity. Many properties of single RNNs are well
                   characterized theoretically, but experimental
                   neuroscience has moved in the direction of studying
                   multiple interacting areas, and RNN theory needs to
                   be likewise extended. We take a constructive approach
                   towards this problem, leveraging tools from nonlinear
                   control theory and machine learning to characterize
                   when combinations of stable RNNs will themselves be
                   stable. Importantly, we derive conditions which allow
                   for massive feedback connections between interacting
                   RNNs. We parameterize these conditions for easy
                   optimization using gradient-based techniques, and
                   show that stability-constrained "networks of
                   networks" can perform well on challenging
                   sequential-processing benchmark tasks. Altogether,
                   our results provide a principled approach towards
                   understanding distributed, modular function in the
                   brain.},
  doi =           {10.48550/arXiv.2106.08928},
}

@article{RSP:59,
  author =        {R. S. Palais},
  journal =       {Transactions of the American Mathematical Society},
  number =        {1},
  pages =         {125--141},
  title =         {Natural operations on differential forms},
  volume =        {92},
  year =          {1959},
  doi =           {10.1090/s0002-9947-1959-0116352-7},
}

@article{IWS-ANW:69,
  author =        {Sandberg, I. W. and Willson, A. N.},
  journal =       {The Bell System Technical Journal},
  number =        {5},
  pages =         {1293-1311},
  title =         {Some network-theoretic properties of nonlinear {DC}
                   transistor networks},
  volume =        {48},
  year =          {1969},
  doi =           {10.1002/j.1538-7305.1969.tb04269.x},
}

@inproceedings{MJT:04,
  author =        {M. J. Tsatsomeros},
  booktitle =     {Focus on Computational Neurobiology},
  editor =        {Lei Li},
  pages =         {115-132},
  publisher =     {Nova Science Publishers},
  title =         {Generating and Detecting Matrices with Positive
                   Principal Minors},
  year =          {2004},
  abstract =      {A brief but concise review of methods to generate
                   P-matrices (i.e., matrices having positive principal
                   minors) is provided and motivated by open problems on
                   P-matrices and the desire to develop and test
                   efficient methods for the detection of P-matrices.
                   Also discussed are operations that leave the class of
                   P-matrices invariant. Some new results and extensions
                   of results regarding P-matrices are included.},
  isbn =          {1590339150},
}

@article{ANM-JAF:90,
  author =        {A. N. {Michel} and J. A. {Farrell}},
  journal =       {IEEE Control Systems Magazine},
  number =        {3},
  pages =         {6-17},
  title =         {Associative memories via artificial neural networks},
  volume =        {10},
  year =          {1990},
  doi =           {10.1109/37.55118},
}

@article{SB-GB-FB-SZ:24m,
  author =        {S. Betteti and G. Baggio and F. Bullo and
                   S. Zampieri},
  journal =       {Neural Computation},
  note =          {To appear},
  title =         {Firing Rate Models as Associative Memory:
                   Excitatory-Inhibitory Balance for Robust Retrieval},
  year =          {2025},
  doi =           {10.48550/arXiv.2411.07388},
}

@article{BAO-DJF:04,
  author =        {B. A. Olshausen and D. J. Field},
  journal =       {Current Opinion in Neurobiology},
  number =        {4},
  pages =         {481--487},
  title =         {Sparse coding of sensory inputs},
  volume =        {14},
  year =          {2004},
  doi =           {10.1016/j.conb.2004.07.007},
}

@article{MP-SC-TN-GL:11,
  author =        {M. Papadopoulou and S. Cassenaer and T. Nowotny and
                   G. Laurent},
  journal =       {Science},
  number =        {6030},
  pages =         {721--725},
  title =         {Normalization for Sparse Encoding of Odors by a
                   Wide-Field Interneuron},
  volume =        {332},
  year =          {2011},
  abstract =      {Sparse coding presents practical advantages for
                   sensory representations and memory storage. In the
                   insect olfactory system, the representation of
                   general odors is dense in the antennal lobes but
                   sparse in the mushroom bodies, only one synapse
                   downstream. In locusts, this transformation relies on
                   the oscillatory structure of antennal lobe output,
                   feed-forward inhibitory circuits, intrinsic
                   properties of mushroom body neurons, and connectivity
                   between antennal lobe and mushroom bodies. Here we
                   show the existence of a normalizing negative-feedback
                   loop within the mushroom body to maintain sparse
                   output over a wide range of input conditions. This
                   loop consists of an identifiable “giant”
                   nonspiking inhibitory interneuron with ubiquitous
                   connectivity and graded release properties.},
  doi =           {10.1126/science.1201835},
}

@article{NP-SB:14,
  author =        {N. Parikh and S. Boyd},
  journal =       {Foundations and Trends in Optimization},
  number =        {3},
  pages =         {127-239},
  title =         {Proximal Algorithms},
  volume =        {1},
  year =          {2014},
  doi =           {10.1561/2400000003},
}

@book{AB:17,
  author =        {A. Beck},
  publisher =     {SIAM},
  title =         {First-Order Methods in Optimization},
  year =          {2017},
  isbn =          {978-1-61197-498-0},
}

@article{AG-AD-FB:24d,
  author =        {A. Gokhale and A. Davydov and F. Bullo},
  journal =       {IEEE Control Systems Letters},
  pages =         {2853-2858},
  title =         {Proximal Gradient Dynamics: {Monotonicity},
                   Exponential Convergence, and Applications},
  volume =        {8},
  year =          {2024},
  doi =           {10.1109/LCSYS.2024.3516632},
}

@article{CJR-DHJ-RGB-BAO:08,
  author =        {C. J. Rozell and D. H. Johnson and R. G. Baraniuk and
                   B. A. Olshausen},
  journal =       {Neural Computation},
  number =        {10},
  pages =         {2526--2563},
  title =         {Sparse Coding via Thresholding and Local Competition
                   in Neural Circuits},
  volume =        {20},
  year =          {2008},
  doi =           {10.1162/neco.2008.03-07-486},
}

@article{PAA-KK:06,
  author =        {P.-A. Absil and K. Kurdyka},
  journal =       {Systems \& Control Letters},
  pages =         {573-577},
  title =         {On the stable equilibrium points of gradient systems},
  volume =        {55},
  year =          {2006},
  abstract =      {This paper studies the relations between the local
                   minima of a cost function f and the stable equilibria
                   of the gradient descent flow of f. In particular, it
                   is shown that, under the assumption that f is real
                   analytic, local minimality is necessary and
                   sufficient for stability. Under the weaker assumption
                   that f is indefinitely continuously differentiable,
                   local minimality is neither necessary nor sufficient
                   for stability.},
  doi =           {10.1016/j.sysconle.2006.01.002},
}

@article{ACBdO-MS-EDS:24,
  author =        {A. C. B. {de~Oliveira} and M. Siami and E. D. Sontag},
  journal =       {arXiv preprint arXiv:2408.15456},
  title =         {Convergence analysis of overparametrized {LQR}
                   formulations},
  year =          {2024},
}

@article{SL:84,
  author =        {S. {\L}ojasiewicz},
  journal =       {Seminari di Geometria 1982-1983},
  note =          {Istituto di Geometria, Dipartimento di Matematica,
                   Universit{\`a} di Bologna, Italy},
  pages =         {115-117},
  title =         {Sur les trajectoires du gradient d'une fonction
                   analytique},
  year =          {1984},
}

@article{PAA-RM-BA:05,
  author =        {P.-A. Absil and R. Mahony and B. Andrews},
  journal =       {SIAM Journal on Control and Optimization},
  number =        {2},
  pages =         {531--547},
  title =         {Convergence of the Iterates of Descent Methods for
                   Analytic Cost Functions},
  volume =        {6},
  year =          {2005},
  doi =           {10.1137/040605266},
}

@article{SG:78,
  author =        {S. Grossberg},
  journal =       {Journal of Mathematical Analysis and Applications},
  number =        {2},
  pages =         {470--493},
  title =         {Competition, decision, and consensus},
  volume =        {66},
  year =          {1978},
  doi =           {10.1016/0022-247x(78)90249-4},
}

@article{SK-TK:94,
  author =        {S. Kaski and T. Kohonen},
  journal =       {Neural Networks},
  number =        {6-7},
  pages =         {973--984},
  title =         {Winner-take-all networks for physiological models of
                   competitive learning},
  volume =        {7},
  year =          {1994},
  doi =           {10.1016/s0893-6080(05)80154-6},
}

@article{YF-MAC-TGK:96,
  author =        {Y. Fang and Michael A. Cohen and Thomas G. Kincaid},
  journal =       {Neural Networks},
  number =        {7},
  pages =         {1141--1154},
  title =         {Dynamics of a Winner-Take-All Neural Network},
  volume =        {9},
  year =          {1996},
  doi =           {10.1016/0893-6080(96)00019-6},
}

@article{PR-EK:05,
  author =        {P. Tymoshchuk and E. Kaszkurewicz},
  journal =       {Neurocomputing},
  note =          {(Trends in Neurocomputing: 12th European Symposium on
                   Artificial Neural Networks 2004)},
  pages =         {375-396},
  title =         {A winner-take-all circuit using neural networks as
                   building blocks},
  volume =        {64},
  year =          {2005},
  abstract =      {A new analog inhibitory method based winner-take-all
                   (WTA) circuit is proposed. The circuit structure is a
                   binary tree arrangement of two-dimensional neural
                   networks and logic nodes. The connection matrix
                   belongs to the class of diagonally stable matrices
                   and the activation functions are piecewise linear or
                   sigmoidal. The mathematical justification of the
                   strong WTA network functioning, on the basis of the
                   uniqueness and global asymptotic stability of the
                   equilibrium, is given. Simulation examples and the
                   corresponding results are also provided.},
  doi =           {10.1016/j.neucom.2004.08.002},
}

@article{SL-BL-YL:13,
  author =        {S. Li and B. Liu and Y. Li},
  journal =       {IEEE Transactions on Neural Networks and Learning
                   Systems},
  number =        {2},
  pages =         {301--309},
  title =         {Selective Positive{\textendash}Negative Feedback
                   Produces the Winner-Take-All Competition in Recurrent
                   Neural Networks},
  volume =        {24},
  year =          {2013},
  doi =           {10.1109/tnnls.2012.2230451},
}

@article{JB-UR-GI-MP:14,
  author =        {J. Binas and U. Rutishauser and G. Indiveri and
                   M. Pfeiffer},
  journal =       {Frontiers in Computational Neuroscience},
  title =         {Learning and stabilization of winner-take-all
                   dynamics through interacting excitatory and
                   inhibitory plasticity},
  volume =        {8},
  year =          {2014},
  abstract =      {Winner-Take-All (WTA) networks are recurrently
                   connected populations of excitatory and inhibitory
                   neurons that represent promising candidate
                   microcircuits for implementing cortical computation.
                   WTAs can perform powerful computations, ranging from
                   signal-restoration to state-dependent processing.
                   However, such networks require fine-tuned
                   connectivity parameters to keep the network dynamics
                   within stable operating regimes. In this article, we
                   show how such stability can emerge autonomously
                   through an interaction of biologically plausible
                   plasticity mechanisms that operate simultaneously on
                   all excitatory and inhibitory synapses of the
                   network. A weight-dependent plasticity rule is
                   derived from the triplet spike-timing dependent
                   plasticity model, and its stabilization properties in
                   the mean-field case are analyzed using contraction
                   theory. Our main result provides simple constraints
                   on the plasticity rule parameters, rather than on the
                   weights themselves, which guarantee stable WTA
                   behavior. The plastic network we present is able to
                   adapt to changing input conditions, and to
                   dynamically adjust its gain, therefore exhibiting
                   self-stabilization mechanisms that are crucial for
                   maintaining stable operation in large networks of
                   interconnected subunits. We show how distributed
                   neural assemblies can adjust their parameters for
                   stable WTA function autonomously while respecting
                   anatomical constraints on neural wiring.},
  doi =           {10.3389/fncom.2014.00068},
}

@article{YL-JL-ZW:19,
  author =        {Y. Li and J. Lu and Z. Wang},
  journal =       {SIAM Journal on Scientific Computing},
  number =        {4},
  pages =         {A2681-A2716},
  title =         {CoordinateWise Descent Methods for Leading Eigenvalue
                   Problem},
  volume =        {41},
  year =          {2019},
  doi =           {10.1137/18m1202505},
}

@unpublished{BE:23,
  author =        {B. Ermentrout},
  month =         dec,
  title =         {Personal Communication},
  year =          {2023},
}

@book{FB:24-LNS,
  author =        {F. Bullo},
  edition =       {{1.7}},
  month =         apr,
  publisher =     {Kindle Direct Publishing},
  title =         {Lectures on Network Systems},
  year =          {2024},
  isbn =          {978-1986425643},
  url =           {https://fbullo.github.io/lns},
}

@article{AB-TRP:93,
  author =        {A. Bouzerdoum and T. R. Pattison},
  journal =       {IEEE Transactions on Neural Networks},
  number =        {2},
  pages =         {293--304},
  title =         {Neural network for quadratic optimization with bound
                   constraints},
  volume =        {4},
  year =          {1993},
  doi =           {10.1109/72.207617},
}

@book{HHB-PLC:17,
  author =        {H. H. Bauschke and P. L. Combettes},
  edition =       {2},
  publisher =     {Springer},
  title =         {Convex Analysis and Monotone Operator Theory in
                   Hilbert Spaces},
  year =          {2017},
  isbn =          {978-3-319-48310-8},
}

@article{VC-AG-AD-GR-FB:23a,
  author =        {V. Centorrino and A. Gokhale and A. Davydov and
                   G. Russo and F. Bullo},
  journal =       {Neural Computation},
  number =        {6},
  pages =         {1163–1197},
  title =         {Positive Competitive Networks for Sparse
                   Reconstruction},
  volume =        {36},
  year =          {2024},
  abstract =      {We propose and analyze a continuous-time firing-rate
                   neural network, the positive firing-rate competitive
                   network, to tackle sparse reconstruction problems
                   with non-negativity constraints. These problems,
                   which involve approximating a given input stimulus
                   from a dictionary using a set of sparse (active)
                   neurons, play a key role in a wide range of domains,
                   including for example neuroscience, signal
                   processing, and machine learning. First, by
                   leveraging the theory of proximal operators, we
                   relate the equilibria of a family of continuous-time
                   firing-rate neural networks to the optimal solutions
                   of sparse reconstruction problems. Then, we prove
                   that our model is a positive system and give rigorous
                   conditions for the convergence to the equilibrium.
                   Specifically, we show that the convergence: (i) only
                   depends on a property of the dictionary; (ii) is
                   linear-exponential, in the sense that initially the
                   convergence rate is at worst linear and then, after a
                   transient, it becomes exponential. We also prove a
                   number of technical results to assess the
                   contractivity properties of the neural dynamics of
                   interest. Our analysis leverages contraction theory
                   to characterize the behavior of a family of
                   firing-rate competitive networks for sparse
                   reconstruction with and without non-negativity
                   constraints. Finally, we validate the effectiveness
                   of our approach via a numerical example.},
  doi =           {10.1162/neco_a_01657},
}

@inproceedings{HK-JN-MS:16,
  author =        {H. Karimi and J. Nutini and M. Schmidt},
  booktitle =     {Machine Learning and Knowledge Discovery in
                   Databases},
  pages =         {795-811},
  publisher =     {Springer},
  title =         {Linear Convergence of Gradient and Proximal-Gradient
                   Methods Under the {Polyak}-Łojasiewicz Condition},
  year =          {2016},
  abstract =      {In 1963, Polyak proposed a simple condition that is
                   sufficient to show a global linear convergence rate
                   for gradient descent. This condition is a special
                   case of the {\L}ojasiewicz inequality proposed in the
                   same year, and it does not require strong convexity
                   (or even convexity). In this work, we show that this
                   much-older Polyak-{\L}ojasiewicz (PL) inequality is
                   actually weaker than the main conditions that have
                   been explored to show linear convergence rates
                   without strong convexity over the last 25 years. We
                   also use the PL inequality to give new analyses of
                   coordinate descent and stochastic gradient for many
                   non-strongly-convex (and some non-convex) functions.
                   We further propose a generalization that applies to
                   proximal-gradient methods for non-smooth
                   optimization, leading to simple proofs of linear
                   convergence for support vector machines and
                   L1-regularized least squares without additional
                   assumptions.},
  doi =           {10.1007/978-3-319-46128-1_50},
  isbn =          {9783319461281},
}

@article{JB-TPN-JP-BWS:16,
  author =        {J. Bolte and T. P. Nguyen and J. Peypouquet and
                   B. W. Suter},
  journal =       {Mathematical Programming},
  number =        {2},
  pages =         {471-507},
  title =         {From error bounds to the complexity of first-order
                   descent methods for convex functions},
  volume =        {165},
  year =          {2016},
  doi =           {10.1007/s10107-016-1091-6},
}

@article{BAO-DJF:96,
  author =        {B. A. Olshausen and D. J. Field},
  journal =       {Nature},
  number =        {6583},
  pages =         {607--609},
  title =         {Emergence of simple-cell receptive field properties
                   by learning a sparse code for natural images},
  volume =        {381},
  year =          {1996},
  doi =           {10.1038/381607a0},
}

@article{AB-JR-CJR:12,
  author =        {A. Balavoine and J. Romberg and C. J. Rozell},
  journal =       {IEEE Transactions on Neural Networks and Learning
                   Systems},
  number =        {9},
  pages =         {1377--1389},
  title =         {Convergence and Rate Analysis of Neural Networks for
                   Sparse Approximation},
  volume =        {23},
  year =          {2012},
  doi =           {10.1109/tnnls.2012.2202400},
}

@article{AB-CJR-JR:13,
  author =        {A. Balavoine and C. J. Rozell and J. Romberg},
  journal =       {IEEE Transactions on Signal Processing},
  number =        {17},
  pages =         {4259--4269},
  title =         {Convergence Speed of a Dynamical System for Sparse
                   Recovery},
  volume =        {61},
  year =          {2013},
  doi =           {10.1109/tsp.2013.2271482},
}

@article{AB-CJR-JR:15,
  author =        {A. Balavoine and C. J. Rozell and J. Romberg},
  journal =       {IEEE Transactions on Signal Processing},
  number =        {12},
  pages =         {3165--3176},
  title =         {Discrete and Continuous-Time Soft-Thresholding for
                   Dynamic Signal Recovery},
  volume =        {63},
  year =          {2015},
  doi =           {10.1109/tsp.2015.2420535},
}

@article{QL-JW:16,
  author =        {Q. Liu and J. Wang},
  journal =       {IEEE Transactions on Neural Networks and Learning
                   Systems},
  number =        {3},
  pages =         {698--707},
  title =         {${L}_1$-Minimization Algorithms for Sparse Signal
                   Reconstruction Based on a Projection Neural Network},
  volume =        {27},
  year =          {2016},
  doi =           {10.1109/tnnls.2015.2481006},
}

@article{YZ-XL-XH-RT:21,
  author =        {Y. Zhao and X. Liao and X. He and R. Tang},
  journal =       {IEEE Transactions on Neural Networks and Learning
                   Systems},
  number =        {12},
  pages =         {7488-7501},
  title =         {Centralized and Collective Neurodynamic Optimization
                   Approaches for Sparse Signal Reconstruction via
                   {$L_1$}-Minimization},
  volume =        {33},
  year =          {2022},
  doi =           {10.1109/tnnls.2021.3085314},
}

@article{HW-XH-TH:22,
  author =        {H. Wen and X. He and T. Huang},
  journal =       {Neural Networks},
  pages =         {1--12},
  title =         {Sparse signal reconstruction via recurrent neural
                   networks with hyperbolic tangent function},
  volume =        {153},
  year =          {2022},
  doi =           {10.1016/j.neunet.2022.05.022},
}

@article{BKN:95,
  author =        {B. K. Natarajan},
  journal =       {{SIAM} Journal on Computing},
  number =        {2},
  pages =         {227--234},
  title =         {Sparse Approximate Solutions to Linear Systems},
  volume =        {24},
  year =          {1995},
  doi =           {10.1137/s0097539792240406},
}

@article{JM-ME-GS:08,
  author =        {J. Mairal and M. Elad and G. Sapiro},
  journal =       {{IEEE} Transactions on Image Processing},
  number =        {1},
  pages =         {53--69},
  title =         {Sparse Representation for Color Image Restoration},
  volume =        {17},
  year =          {2008},
  doi =           {10.1109/tip.2007.911828},
}

@article{EJC-JKR-TT:06,
  author =        {E. J. Cand{\`{e}}s and J. K. Romberg and T. Tao},
  journal =       {Communications on Pure and Applied Mathematics},
  number =        {8},
  pages =         {1207--1223},
  title =         {Stable signal recovery from incomplete and inaccurate
                   measurements},
  volume =        {59},
  year =          {2006},
  doi =           {10.1002/cpa.20124},
}

@article{DLD:06,
  author =        {D. L. Donoho},
  journal =       {IEEE Transactions on Information Theory},
  number =        {4},
  pages =         {1289--1306},
  title =         {Compressed sensing},
  volume =        {52},
  year =          {2006},
  doi =           {10.1109/tit.2006.871582},
}

@article{EJC:08,
  author =        {E. J. Candès},
  journal =       {Comptes Rendus Mathematique},
  number =        {9},
  pages =         {589-592},
  title =         {The restricted isometry property and its implications
                   for compressed sensing},
  volume =        {346},
  year =          {2008},
  abstract =      {It is now well-known that one can reconstruct sparse
                   or compressible signals accurately from a very
                   limited number of measurements, possibly contaminated
                   with noise. This technique known as “compressed
                   sensing” or “compressive sampling” relies on
                   properties of the sensing matrix such as the
                   restricted isometry property. In this Note, we
                   establish new results about the accuracy of the
                   reconstruction from undersampled measurements which
                   improve on earlier estimates, and have the advantage
                   of being more elegant.},
  doi =           {10.1016/j.crma.2008.03.014},
}

@book{SF-HR:13,
  author =        {S. Foucart and H. Rauhut},
  publisher =     {Springer},
  title =         {A Mathematical Introduction to Compressive Sensing},
  year =          {2013},
  doi =           {10.1007/978-0-8176-4948-7},
}

@article{RJTib:13,
  author =        {R. J. Tibshirani},
  journal =       {Electronic Journal of Statistics},
  publisher =     {Institute of Mathematical Statistics},
  title =         {The lasso problem and uniqueness},
  volume =        {7},
  year =          {2013},
  doi =           {10.1214/13-ejs815},
}

@article{JJH-DWT:85,
  author =        {J. J. Hopfield and D. W. Tank},
  journal =       {Biological Cybernetics},
  number =        {3},
  pages =         {141--152},
  title =         {{"Neural"} computation of decisions in optimization
                   problems},
  volume =        {52},
  year =          {1985},
  doi =           {10.1007/bf00339943},
}

@article{MPK-LOC:88,
  author =        {M. P. Kennedy and L. O. Chua},
  journal =       {IEEE Transactions on Circuits and Systems},
  number =        {5},
  pages =         {554--562},
  publisher =     {Institute of Electrical and Electronics Engineers
                   ({IEEE})},
  title =         {Neural networks for nonlinear programming},
  volume =        {35},
  year =          {1988},
  doi =           {10.1109/31.1783},
}

@article{SHM-MRJ:21,
  author =        {S. Hassan-Moghaddam and M. R. Jovanovi{\'c}},
  journal =       {Automatica},
  pages =         {109311},
  title =         {Proximal gradient flow and {D}ouglas-{R}achford
                   splitting dynamics: {G}lobal exponential stability
                   via integral quadratic constraints},
  volume =        {123},
  year =          {2021},
  doi =           {10.1016/j.automatica.2020.109311},
}

@article{JJF:04,
  author =        {Fuchs, J.-J.},
  journal =       {IEEE Transactions on Information Theory},
  number =        {6},
  pages =         {1341-1344},
  title =         {On sparse representations in arbitrary redundant
                   bases},
  volume =        {50},
  year =          {2004},
  doi =           {10.1109/TIT.2004.828141},
}

@article{AD-VC-AG-GR-FB:23f,
  author =        {A. Davydov and V. Centorrino and A. Gokhale and
                   G. Russo and F. Bullo},
  journal =       {IEEE Transactions on Automatic Control},
  note =          {To appear},
  number =        {11},
  title =         {Time-Varying Convex Optimization: A Contraction and
                   Equilibrium Tracking Approach},
  volume =        {70},
  year =          {2025},
  abstract =      {In this article, we provide a novel and
                   broadly-applicable contraction-theoretic approach to
                   continuous-time time-varying convex optimization. For
                   any parameter-dependent contracting dynamics, we show
                   that the tracking error is asymptotically
                   proportional to the rate of change of the parameter
                   and that the proportionality constant is upper
                   bounded by Lipschitz constant in which the parameter
                   appears divided by the contraction rate of the
                   dynamics squared. We additionally establish that
                   augmenting any parameter-dependent contracting
                   dynamics with a feedforward prediction term ensures
                   that the tracking error vanishes exponentially
                   quickly. To apply these results to time-varying
                   convex optimization, we establish the strong
                   infinitesimal contractivity of dynamics solving three
                   canonical problems: monotone inclusions, linear
                   equality-constrained problems, and composite
                   minimization problems. For each case, we derive the
                   sharpest-known contraction rates and provide explicit
                   bounds on the tracking error between solution
                   trajectories and minimizing trajectories. We validate
                   our theoretical results on two numerical examples
                   \change{and on an} application to control barrier
                   function-based controller design that involves real
                   hardware.},
  doi =           {10.1109/TAC.2025.3576043},
}

@techreport{KBP-MSP:12,
  author =        {Petersen, K. B. and M. S. Pedersen},
  institution =   {Technical University of Denmark},
  month =         nov,
  title =         {The Matrix Cookbook},
  year =          {2012},
  url =           {https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf},
}

@article{XL-ZW-YZ:15,
  author =        {X. Liu and Z. Wen and Y. Zhang},
  journal =       {SIAM Journal on Optimization},
  number =        {3},
  pages =         {1571-1608},
  title =         {An Efficient {Gauss--Newton} Algorithm for Symmetric
                   Low-Rank Product Matrix Approximations},
  volume =        {25},
  year =          {2015},
  doi =           {10.1137/140971464},
}

@article{scikit-learn:11,
  author =        {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and
                   Michel, V. and Thirion, B. and Grisel, O. and
                   Blondel, M. and Prettenhofer, P. and Weiss, R. and
                   Dubourg, V. and Vanderplas, J. and Passos, A. and
                   Cournapeau, D. and Brucher, M. and Perrot, M. and
                   Duchesnay, E.},
  journal =       {Journal of Machine Learning Research},
  pages =         {2825--2830},
  title =         {Scikit-learn: Machine Learning in {P}ython},
  volume =        {12},
  year =          {2011},
}

@article{TDS:89,
  author =        {T. D. Sanger},
  journal =       {Neural Networks},
  number =        {6},
  pages =         {459--473},
  title =         {Optimal unsupervised learning in a single-layer
                   linear feedforward neural network},
  volume =        {2},
  year =          {1989},
  doi =           {10.1016/0893-6080(89)90044-0},
}

@inproceedings{PF:89,
  address =       {Washington DC},
  author =        {P. F\"{o}ldi{\'{a}}k},
  booktitle =     {International Joint Conference on Neural Networks},
  pages =         {401-406},
  title =         {Adaptive network for optimal linear feature
                   extraction},
  year =          {1989},
  doi =           {10.1109/ijcnn.1989.118615},
}

@article{PF:90,
  author =        {P. F\"{o}ldi{\'{a}}k},
  journal =       {Biological Cybernetics},
  number =        {2},
  pages =         {165-70},
  title =         {Forming sparse representations by local
                   anti-{Hebbian} learning},
  volume =        {64},
  year =          {1990},
  doi =           {10.1007/BF02331346},
}

@incollection{KID:02,
  author =        {K. I. Diamantaras},
  booktitle =     {Handbook of Neural Network Signal Processing},
  editor =        {Yu Hen Hu and Jenq-Neng Hwang},
  note =          {Chapter~8},
  publisher =     {CRC Press},
  title =         {Neural networks and principal component analysis},
  year =          {2018},
  doi =           {10.1201/9781315220413},
  isbn =          {9781315220413},
}

@inproceedings{TH-CP-DBC:14,
  author =        {T. Hu and C. Pehlevan and D. B. Chklovskii},
  booktitle =     {Asilomar Conference on Signals, Systems and
                   Computers},
  pages =         {613--619},
  title =         {A {Hebbian}/{Anti-Hebbian} Network for Online Sparse
                   Dictionary Learning Derived from Symmetric Matrix
                   Factorization},
  year =          {2014},
  doi =           {10.1109/ACSSC.2014.7094519},
}

@article{CP-TH-DBC:15,
  author =        {C. Pehlevan and T. Hu and D. B. Chklovskii},
  journal =       {Neural Computation},
  number =        {7},
  pages =         {1461--1495},
  title =         {A {Hebbian}/{Anti-Hebbian} Neural Network for Linear
                   Subspace Learning: {A} Derivation from
                   Multidimensional Scaling of Streaming Data},
  volume =        {27},
  year =          {2015},
  doi =           {10.1162/neco_a_00745},
}

@article{CP-AMS-DBC:17,
  author =        {C. Pehlevan and A. M. Sengupta and D. B. Chklovskii},
  journal =       {Neural Computation},
  number =        {1},
  pages =         {84--124},
  title =         {Why do similarity matching objectives lead to
                   {Hebbian}/anti-{Hebbian} networks?},
  volume =        {30},
  year =          {2017},
  abstract =      {Modeling self-organization of neural networks for
                   unsupervised learning using Hebbian and anti-Hebbian
                   plasticity has a long history in neuroscience. Yet
                   derivations of single-layer networks with such local
                   learning rules from principled optimization
                   objectives became possible only recently, with the
                   introduction of similarity matching objectives. What
                   explains the success of similarity matching
                   objectives in deriving neural networks with local
                   learning rules? Here, using dimensionality reduction
                   as an example, we introduce several variable
                   substitutions that illuminate the success of
                   similarity matching. We show that the full network
                   objective may be optimized separately for each
                   synapse using local learning rules in both the
                   offline and online settings. We formalize the
                   long-standing intuition of the rivalry between
                   Hebbian and anti-Hebbian rules by formulating a
                   min-max optimization problem. We introduce a novel
                   dimensionality reduction objective using fractional
                   matrix exponents. To illustrate the generality of our
                   approach, we apply it to a novel formulation of
                   dimensionality reduction combined with whitening. We
                   confirm numerically that the networks with learning
                   rules derived from principled objectives perform
                   better than those with heuristic learning rules.},
  doi =           {10.1162/neco_a_01018},
}

@article{VC-FB-GR:24k,
  author =        {V. Centorrino and F. Bullo and G. Russo},
  journal =       {Neural Computation},
  month =         jun,
  note =          {Submitted},
  title =         {Similarity Matching Networks: {Hebbian} Learning and
                   Convergence over Multiple Time Scales},
  year =          {2025},
  abstract =      {A recent breakthrough in biologically-plausible
                   normative frameworks for dimensionality reduction is
                   based upon the similarity matching cost function and
                   the low-rank matrix approximation problem. Despite
                   clear biological interpretation, successful
                   application in several domains, and experimental
                   validation, a formal complete convergence analysis
                   remains elusive. Building on this framework, we
                   consider and analyze a continuous-time neural
                   network, the \emph{similarity matching network}, for
                   principal subspace projection. Derived from a
                   min-max-min objective, this biologically-plausible
                   network consists of three coupled dynamics evolving
                   at different time scales: neural dynamics, lateral
                   synaptic dynamics, and feedforward synaptic dynamics
                   at the fast, intermediate, and slow time scales,
                   respectively. The feedforward and lateral synaptic
                   dynamics consist of Hebbian and anti-Hebbian learning
                   rules, respectively. By leveraging a multilevel
                   optimization framework, we prove convergence of the
                   dynamics in the offline setting. Specifically, at the
                   first level (fast time scale), we show strong
                   convexity of the cost function and global exponential
                   convergence of the corresponding gradient-flow
                   dynamics. At the second level (intermediate time
                   scale), we prove strong concavity of the cost
                   function and exponential convergence of the
                   corresponding gradient-flow dynamics within the space
                   of positive definite matrices. At the third and final
                   level (slow time scale), we study a non-convex and
                   non-smooth cost function, provide explicit
                   expressions for its global minima, and prove almost
                   sure convergence of the corresponding gradient-flow
                   dynamics to the global minima. These results rely
                   upon two mild technical conjectures that we support
                   empirically. Finally, we validate the effectiveness
                   of our approach via a numerical example.},
  doi =           {10.48550/arXiv.2506.06134},
}

@inproceedings{JMQ:67,
  author =        {J. MacQueen},
  booktitle =     {Fifth Berkeley Symposium on Mathematical Statistics,
                   and Probability},
  pages =         {281--297},
  title =         {Some methods for classification and analysis of
                   multivariate observations},
  year =          {1967},
}

@article{JSB-ALV:90,
  author =        {J. S. Baras and A. LaVigna},
  journal =       {Advances in Neural Information Processing Systems},
  title =         {Convergence of a neural network classifier},
  volume =        {3},
  year =          {1990},
}

@article{EY-KZ-AG:92,
  author =        {E. Yair and K. Zeger and A. Gersho},
  journal =       {IEEE Transactions on Signal Processing},
  number =        {2},
  pages =         {294--309},
  title =         {Competitive learning and soft competition for vector
                   quantizer design},
  volume =        {40},
  year =          {1992},
  doi =           {10.1109/78.124940},
}

@article{KLD:10,
  author =        {K.-L. Du},
  journal =       {Neural Networks},
  number =        {1},
  pages =         {89--107},
  title =         {Clustering: {A} neural network approach},
  volume =        {23},
  year =          {2010},
  doi =           {10.1016/j.neunet.2009.08.007},
}

@article{CNM-JSB:20,
  author =        {C. N. Mavridis and J. S. Baras},
  journal =       {IFAC-PapersOnLine},
  note =          {IFAC World Congress},
  number =        {2},
  pages =         {2214-2219},
  title =         {Convergence of Stochastic Vector Quantization and
                   Learning Vector Quantization with {Bregman}
                   Divergences},
  volume =        {53},
  year =          {2020},
  abstract =      {Stochastic vector quantization methods have been
                   extensively studied in supervised and unsupervised
                   learning problems as online, data-driven,
                   interpretable, robust, and fast to train and evaluate
                   algorithms. Being prototype-based methods, they
                   depend on a dissimilarity measure, which is both
                   necessary and sufficient to belong to the family of
                   Bregman divergences, if the mean value is used as the
                   representative of the cluster. In this work, we
                   investigate the convergence properties of stochastic
                   vector quantization (VQ) and its supervised
                   counterpart, Learning Vector Quantization (LVQ),
                   using Bregman divergences. We employ the theory of
                   stochastic approximation to study the conditions on
                   the initialization and the Bregman divergence
                   generating functions, under which, the algorithms
                   converge to desired configurations. These results
                   formally support the use of Bregman divergences, such
                   as the Kullback-Leibler divergence, in vector
                   quantization algorithms.},
  doi =           {10.1016/j.ifacol.2020.12.006},
}

@article{TK:90,
  author =        {Kohonen, T.},
  journal =       {Proceedings of the IEEE},
  number =        {9},
  pages =         {1464-1480},
  title =         {The self-organizing map},
  volume =        {78},
  year =          {1990},
  doi =           {10.1109/5.58325},
}

@book{MdB-MvK-MO-OS:00,
  author =        {M. de Berg and M. van Kreveld and M. Overmars and
                   O. Schwarzkopf},
  edition =       {2},
  publisher =     {Springer},
  title =         {Computational Geometry: Algorithms and Applications},
  year =          {2000},
  isbn =          {3540656200},
}

@book{FB-JC-SM:09,
  author =        {F. Bullo and J. Cort{\'e}s and S. Mart{\'\i}nez},
  publisher =     {Princeton University Press},
  title =         {Distributed Control of Robotic Networks},
  year =          {2009},
  isbn =          {978-0-691-14195-4},
  url =           {https://fbullo.github.io/dcrn},
}

@article{QD-VF-MG:99,
  author =        {Q. Du and V. Faber and M. Gunzburger},
  journal =       {SIAM Review},
  number =        {4},
  pages =         {637-676},
  title =         {Centroidal {V}oronoi tessellations: {A}pplications
                   and algorithms},
  volume =        {41},
  year =          {1999},
  abstract =      {A centroidal Voronoi tessellation is a Voronoi
                   tessellation whose generating points are the
                   centroids (centers of mass) of the corresponding
                   Voronoi regions. We give some applications of such
                   tessellations to problems in image compression,
                   quadrature, finite difference methods, distribution
                   of resources, cellular biology, statistics, and the
                   territorial behavior of animals. We discuss methods
                   for computing these tessellations, provide some
                   analyses concerning both the tessellations and the
                   methods for their determination, and, finally,
                   present the results of some numerical experiments.
                   (64 References).},
  doi =           {10.1137/S0036144599352836},
}

@article{EO:89,
  author =        {E. Oja},
  journal =       {International Journal of Neural Systems},
  number =        {01},
  pages =         {61--68},
  title =         {Neural Networks, Principal Components, and Subspaces},
  volume =        {01},
  year =          {1989},
  doi =           {10.1142/s0129065789000475},
}

@article{SY-UH-KS:01,
  author =        {S. Yoshizawa and U. Helmke and K. Starkov},
  journal =       {International Journal of Applied Mathematics and
                   Computer Science},
  number =        {1},
  pages =         {223-236},
  title =         {Convergence analysis for principal component flows},
  volume =        {11},
  year =          {2001},
  abstract =      {A common framework for analyzing the global
                   convergence of several flows for principal component
                   analysis is developed. It is shown that flows
                   proposed by Brockett, Oja, Xu and others are all
                   gradient flows and the global convergence of these
                   flows to single equilibrium points is established.
                   The signature of the Hessian at each critical point
                   is determined.},
  url =           {http://eudml.org/doc/207501},
}

@article{JLWJ-IME:95,
  author =        {J. L. {Wyatt~Jr.} and I. M. Elfadel},
  journal =       {Neural Computation},
  number =        {5},
  pages =         {915-922},
  title =         {Time-Domain Solutions of {Oja's} Equations},
  volume =        {7},
  year =          {1995},
  doi =           {10.1162/neco.1995.7.5.915},
}

@article{WYY-UH-JBM:94,
  author =        {W.-Y. Yan and U. Helmke and J. B. Moore},
  journal =       {IEEE Transactions on Neural Networks},
  number =        {5},
  pages =         {674-683},
  title =         {Global analysis of {Oja}'s flow for neural networks},
  volume =        {5},
  year =          {1994},
  abstract =      {A detailed study of Oja's learning equation in neural
                   networks is undertaken in this paper. Not only are
                   such fundamental issues as existence, uniqueness, and
                   representation of solutions completely resolved, but
                   also the convergence issue is resolved. It is shown
                   that the solution of Oja's equation is exponentially
                   convergent to an equilibrium from any initial value.
                   Moreover, the necessary and sufficient conditions are
                   given on the initial value for the solution to
                   converge to a dominant eigenspace of the associated
                   autocorrelation matrix. As a by-product, this result
                   confirms one of Oja's conjectures that the solution
                   converges to the principal eigenspace from almost all
                   initial values. Some other characteristics of the
                   limiting solution are also revealed. These facilitate
                   the determination of the limiting solution in advance
                   using only the initial information. Two examples are
                   analyzed demonstrating the explicit dependence of the
                   limiting solution on the initial value. In another
                   respect, it is found that Oja's equation is the
                   gradient flow of generalized Rayleigh quotients on a
                   Stiefel manifold.},
  doi =           {10.1109/72.317720},
}

@article{REM-HH-JBM:96,
  author =        {Mahony, R. E. and Helmke, U. and Moore, J. B.},
  journal =       {ANZIAM Journal},
  number =        {4},
  pages =         {430-450},
  publisher =     {Cambridge University Press},
  title =         {Gradient algorithms for principal component analysis},
  volume =        {37},
  year =          {1996},
  doi =           {10.1017/S033427000001078X},
}

@article{BY:95,
  author =        {B. Yang},
  journal =       {IEEE Transactions on Signal Processing},
  number =        {1},
  pages =         {95--107},
  title =         {Projection approximation subspace tracking},
  volume =        {43},
  year =          {1995},
  doi =           {10.1109/78.365290},
}

@article{CP-DBC:15,
  author =        {C. Pehlevan and D. B. Chklovskii},
  journal =       {Advances in Neural Information Processing Systems},
  title =         {A normative theory of adaptive dimensionality
                   reduction in neural networks},
  volume =        {28},
  year =          {2015},
}

@article{CP-SM-DBC:17,
  author =        {C. Pehlevan and S. Mohan and D. B. Chklovskii},
  journal =       {Neural Computation},
  number =        {11},
  pages =         {2925--2954},
  title =         {Blind nonnegative source separation using biological
                   neural networks},
  volume =        {29},
  year =          {2017},
  doi =           {10.1162/neco_a_01007},
}

@inproceedings{ATE-CP:20,
  author =        {A. T. Erdogan and C. Pehlevan},
  booktitle =     {IEEE International Conference on Acoustics, Speech
                   and Signal Processing},
  month =         may,
  pages =         {3812-3816},
  title =         {Blind Bounded Source Separation Using Neural Networks
                   with Local Learning Rules},
  year =          {2020},
  doi =           {10.1109/icassp40776.2020.9053114},
}

@article{BB-CP-AE:22,
  author =        {B. Bozkurt and C. Pehlevan and A. Erdogan},
  journal =       {Advances in Neural Information Processing Systems},
  pages =         {13704--13717},
  title =         {Biologically-plausible determinant maximization
                   neural networks for blind separation of correlated
                   sources},
  volume =        {35},
  year =          {2022},
  doi =           {10.48550/arXiv.2209.12894},
}

@article{DL-CP-DBC:22,
  author =        {D. Lipshutz and C. Pehlevan and D. B. Chklovskii},
  journal =       {Biological Cybernetics},
  pages =         {557-568},
  title =         {Biologically plausible single-layer networks for
                   nonnegative independent component analysis},
  volume =        {116},
  year =          {2022},
  doi =           {10.1007/s00422-022-00943-8},
}

@article{NMC-CP-DBC:23,
  author =        {N. M. Chapochnikov and C. Pehlevan and
                   D. B. Chklovskii},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {29},
  title =         {Normative and mechanistic model of an adaptive
                   circuit for efficient encoding and feature
                   extraction},
  volume =        {120},
  year =          {2023},
  abstract =      {The brain represents information with neural activity
                   patterns. At the periphery, these patterns contain
                   correlations, which are detrimental to stimulus
                   discrimination. We study the peripheral olfactory
                   circuit of the Drosophila larva, which preprocesses
                   neural representations before relaying them
                   downstream. A comprehensive understanding of this
                   preprocessing is, however, lacking. We formulate a
                   principle-driven framework based on
                   similarity-matching and, using neural input activity,
                   derive a circuit model that largely explains the
                   biological circuit's synaptic organization. It also
                   predicts that inhibitory neurons cluster odors and
                   facilitate decorrelation and normalization of neural
                   representations. If equipped with Hebbian synaptic
                   plasticity, the circuit model autonomously adapts to
                   different environments. Our work provides a
                   comprehensive approach to deciphering the
                   relationship between structure and function in neural
                   circuits.},
  doi =           {10.1073/pnas.2117484120},
}

@article{VC-FB-GR:22k,
  author =        {V. Centorrino and F. Bullo and G. Russo},
  journal =       {Automatica},
  pages =         {111636},
  title =         {Modelling and Contractivity of Neural-Synaptic
                   Networks with {Hebbian} Learning},
  volume =        {164},
  year =          {2024},
  doi =           {10.1016/j.automatica.2024.111636},
}

@article{VB-SM:51,
  author =        {H. Robbins and S. Monro},
  journal =       {Annals of Mathematical Statistics},
  number =        {3},
  pages =         {400--407},
  title =         {A Stochastic Approximation Method},
  volume =        {22},
  year =          {1951},
  url =           {http://www.jstor.org/stable/2236626},
}

@article{LL:77,
  author =        {Ljung, L.},
  journal =       {IEEE Transactions on Automatic Control},
  number =        {4},
  pages =         {551-575},
  title =         {Analysis of recursive stochastic algorithms},
  volume =        {22},
  year =          {1977},
  abstract =      {Recursive algorithms where random observations enter
                   are studied in a fairly general framework. An
                   important feature is that the observations my depend
                   on previous "outputs" of the algorithm. The
                   considered class of algorithms contains, e.g.,
                   stochastic approximation algorithm, recursive
                   identification algorithm, and algorithms for adaptive
                   control of linear systems. It is shown how a
                   deterministic differential equation can be associated
                   with the algorithm. Problems like convergence with
                   probability one, possible convergence points and
                   asymptotic behavior of the algorithm can all be
                   studied in terms of this differential equation.
                   Theorems stating the precise relationships between
                   the differential equation and the algorithm are given
                   as well as examples of applications of the results to
                   problems in identification and adaptive control.},
  doi =           {10.1109/TAC.1977.1101561},
}

@inbook{MV:24,
  author =        {M. Vidyasagar},
  booktitle =     {Probability and Stochastic Processes},
  pages =         {177-200},
  publisher =     {Springer},
  title =         {Convergence of Stochastic Approximation via
                   {Martingale} and Converse {Lyapunov} Methods},
  year =          {2024},
  doi =           {10.1007/978-981-99-9994-1_8},
  isbn =          {9789819999941},
}

@article{MTC:88,
  author =        {M. T. Chu},
  journal =       {SIAM Review},
  number =        {3},
  pages =         {375-387},
  title =         {On the Continuous Realization of Iterative Processes},
  volume =        {30},
  year =          {1988},
  abstract =      {Many important mathematical problems are solved by
                   iterative methods. Many of these iterative schemes
                   may be regarded as the discrete realization of
                   certain continuous dynamical systems. This paper
                   summarizes some of the recent developments in the
                   continuous realization of several popular basic
                   iterative methods.},
  doi =           {10.1137/1030090},
}

@article{GG-SK:92,
  author =        {G. Giorgi and S. Koml{\'o}si},
  journal =       {Rivista di Matematica Per Le Scienze Economiche e
                   Sociali},
  number =        {1},
  pages =         {3--30},
  title =         {Dini derivatives in optimization --- {Part I}},
  volume =        {15},
  year =          {1992},
  doi =           {10.1007/BF02086523},
}

@article{JMD:66,
  author =        {J. M. Danskin},
  journal =       {SIAM Journal on Applied Mathematics},
  number =        {4},
  pages =         {641-664},
  title =         {The Theory of Max-Min, with Applications},
  volume =        {14},
  year =          {1966},
  doi =           {10.1137/0114053},
}

@book{JLL:1788,
  address =       {Paris},
  author =        {Joseph Louis Lagrange},
  publisher =     {Chez la Veuve Desaint},
  title =         {M\'ecanique Analytique},
  year =          {1788},
}

@article{JCM:1868,
  author =        {J. C. Maxwell},
  journal =       {Proceedings of the Royal Society. London. Series A.
                   Mathematical and Physical Sciences},
  pages =         {270-283},
  title =         {On Governors},
  volume =        {16},
  year =          {1868},
  doi =           {10.1098/rspl.1867.0055},
}

@book{WT-PGT:1867,
  author =        {W. Thomson and P. G. Tait},
  publisher =     {Oxford University Press},
  title =         {Treatise on Natural Philosophy},
  year =          {1867},
}

@book{AML:1892,
  address =       {Kharkov},
  author =        {Aleksandr Mikhailovich Lyapunov},
  note =          {Translation:~\citep{AML:1992}},
  publisher =     {Fakul\cprime{}teta i Khar\cprime{}kovskogo
                   Matematicheskogo Obshchestva},
  title =         {Ob\v{s}\v{c}aya zada\v{c}a ob usto\u{\i}\v{c}ivosti
                   dvi\v{z}eniya},
  year =          {1892},
}

@article{EAB-NNK:52,
  author =        {E. A. Barbashin and N. N. Krasovski\u{\i}},
  journal =       {Doklady Akademii Nauk SSSR},
  note =          {(In Russian)},
  number =        {3},
  pages =         {453-456},
  title =         {On Global Stability of Motion},
  volume =        {86},
  year =          {1952},
}

@book{NNK:63,
  author =        {N. N. Krasovski\u\i},
  publisher =     {Stanford University Press},
  title =         {Stability of Motion. Applications of Lyapunov's
                   Second Method to Differential Systems and Equations
                   with Delay},
  year =          {1963},
}

@article{JPL:60,
  author =        {J. P. LaSalle},
  journal =       {IRE Transactions on Circuit Theory},
  pages =         {520-527},
  title =         {Some extensions of {L}iapunov's second method},
  volume =        {CT-7},
  year =          {1960},
  doi =           {10.1109/TCT.1960.1086720},
}

@article{JPL:68,
  author =        {J. P. LaSalle},
  journal =       {Journal of Differential Equations},
  pages =         {57--65},
  title =         {Stability Theory for Ordinary Differential Equations},
  volume =        {4},
  year =          {1968},
  doi =           {10.1016/0022-0396(68)90048-X},
}

@book{JPL:76,
  author =        {J. P. LaSalle},
  publisher =     {SIAM},
  title =         {The Stability of Dynamical Systems},
  year =          {1976},
  doi =           {10.1137/1.9781611970432},
  isbn =          {9780898710229},
}

@book{NGC:61,
  author =        {Nikolai Gurevich Chetaev},
  note =          {Translation from Russian by M.~Nadler},
  publisher =     {Pergamon},
  title =         {The Stability of Motion},
  year =          {1961},
}

@book{WH:67,
  author =        {W. Hahn},
  publisher =     {Springer},
  title =         {Stability of Motion},
  year =          {1967},
  isbn =          {978-3-642-50085-5},
}

@book{EDS:98,
  author =        {E. D. Sontag},
  edition =       {2},
  publisher =     {Springer},
  title =         {Mathematical Control Theory: Deterministic Finite
                   Dimensional Systems},
  year =          {1998},
  isbn =          {0387984895},
}

@book{HKK:02,
  author =        {H. K. Khalil},
  edition =       {3},
  publisher =     {Prentice Hall},
  title =         {Nonlinear Systems},
  year =          {2002},
  isbn =          {0130673897},
}

@article{MV:78,
  author =        {M. Vidyasagar},
  journal =       {Journal of Mathematical Analysis and Applications},
  number =        {1},
  pages =         {90-103},
  title =         {On matrix measures and convex {{Liapunov}} functions},
  volume =        {62},
  year =          {1978},
  abstract =      {In this paper, we extend the concept of the measure
                   of a matrix to encompass a measure induced by an
                   arbitrary convex positive definite function. It is
                   shown that this modified matrix measure has most of
                   the properties of the usual matrix measure, and that
                   many of the known applications of the usual matrix
                   measure can therefore be carried over to the modified
                   matrix measure. These applications include deriving
                   conditions for a mapping to be a diffeomorphism on
                   Rn, and estimating the solution errors that result
                   when a nonlinear network is approximated by a
                   piecewise linear network. We also develop a
                   connection between matrix measures and {Liapunov}
                   functions. Specifically, we show that if V is a
                   convex positive definite function and A is a Hurwitz
                   matrix, then mu V(A) < 0, if and only if V is a
                   {Liapunov} function for the system x... = Ax. This
                   linking up between matrix measures and {Liapunov}
                   functions leads to some results on the existence of a
                   “common” matrix measure μV(·) such that μV(Ai)
                   < 0 for each of a given set of matrices A1,…, Am.
                   Finally, we also give some results for matrices with
                   nonnegative off-diagonal terms.},
  doi =           {10.1016/0022-247X(78)90221-4},
}

@book{WMH-SS:74,
  author =        {M. W. Hirsch and S. Smale},
  publisher =     {Academic Press},
  title =         {Differential Equations, Dynamical Systems and Linear
                   Algebra},
  year =          {1974},
  isbn =          {0123495504},
}

@book{VIA:92,
  author =        {Vladimir I. Arnol'd},
  note =          {Translation of the third Russian edition by R.~Cooke},
  publisher =     {Springer},
  title =         {Ordinary Differential Equations},
  year =          {1992},
  isbn =          {3-540-54813-0},
}

@book{JG-PH:90,
  author =        {J. Guckenheimer and P. Holmes},
  publisher =     {Springer},
  title =         {Nonlinear Oscillations, Dynamical Systems, and
                   Bifurcations of Vector Fields},
  year =          {1990},
  isbn =          {0387908196},
}

@book{WMH-VC:08,
  author =        {W. M. Haddad and V. Chellaboina},
  publisher =     {Princeton University Press},
  title =         {Nonlinear Dynamical Systems and Control: A
                   Lyapunov-Based Approach},
  year =          {2008},
  isbn =          {9780691133294},
}

@book{RG-RGS-ART:12,
  author =        {R. Goebel and R. G. Sanfelice and A. R. Teel},
  publisher =     {Princeton University Press},
  title =         {Hybrid Dynamical Systems: Modeling, Stability, and
                   Robustness},
  year =          {2012},
  isbn =          {9780691153896},
}

@book{FHC-YSL-RJS-PRW:98,
  author =        {F. H. Clarke and Y.S. Ledyaev and R. J. Stern and
                   P. R. Wolenski},
  publisher =     {Springer},
  title =         {Nonsmooth Analysis and Control Theory},
  year =          {1998},
  isbn =          {0387983368},
}

@article{JC:08-csm,
  author =        {J. Cort{\'e}s},
  journal =       {{IEEE} Control Systems},
  number =        {3},
  pages =         {36-73},
  title =         {Discontinuous dynamical systems},
  volume =        {28},
  year =          {2008},
  abstract =      {This paper considers discontinuous dynamical systems,
                   i.e., systems whose associated vector field is a
                   discontinuous function of the state. Discontinuous
                   dynamical systems arise in a large number of
                   applications, including optimal control, nonsmooth
                   mechanics, and robotic manipulation. Independently of
                   the particular application, one always faces similar
                   questions when dealing with discontinuous dynamical
                   systems. The most basic one is the notion of
                   solution. We begin by introducing the notions of
                   Caratheodory, Filippov and sample-and-hold solutions,
                   discuss existence and uniqueness results for them,
                   and examine various examples. We also give specific
                   pointers to other notions of solution defined in the
                   literature. Once the notion of solution has been
                   settled, we turn our attention to the analysis of
                   stability of discontinuous systems. We introduce the
                   concepts of generalized gradient of locally Lipschitz
                   functions and proximal subdifferential of lower
                   semicontinuous functions. Building on these notions,
                   we establish monotonic properties of candidate
                   Lyapunov functions along the solutions. These results
                   are key in providing suitable generalizations of
                   Lyapunov stability theorems and the LaSalle
                   Invariance Principle. We illustrate the applicability
                   of these results in a class of nonsmooth gradient
                   flows.},
  doi =           {10.1109/MCS.2008.919306},
}

@article{ZL-BF-MM:07,
  author =        {Z. Lin and B. Francis and M. Maggiore},
  journal =       {SIAM Journal on Control and Optimization},
  number =        {1},
  pages =         {288-307},
  title =         {State agreement for continuous-time coupled nonlinear
                   systems},
  volume =        {46},
  year =          {2007},
  abstract =      {Two related problems are treated in continuous time.
                   First, the state agreement problem is studied for
                   coupled nonlinear differential equations. The vector
                   fields can switch within a finite family. Associated
                   to each vector field is a directed graph based in a
                   natural way on the interaction structure of the
                   subsystems. Generalizing the work of Moreau, under
                   the assumption that the vector fields satisfy a
                   certain subtangentiality condition, it is proved that
                   asymptotic state agreement is achieved if and only if
                   the dynamic interaction digraph has the property of
                   being sufficiently connected over time. The proof
                   uses nonsmooth analysis. Second, the rendezvous
                   problem for kinematic point-mass mobile robots is
                   studied when the robotsÃ fields of view have a
                   fixed radius. The circumcenter control law of Ando et
                   al. [IEEE Trans. Robotics Automation, 15 (1999), pp.
                   818Ã 828] is shown to solve the problem. The
                   rendezvous problem is a kind of state agreement
                   problem, but the interaction structure is state
                   dependent.},
  doi =           {10.1137/050626405},
}

@book{AML:1992,
  author =        {Aleksandr Mikhailovich Lyapunov},
  note =          {Translation from Russian by A.~T.~Fuller},
  publisher =     {Taylor \& Francis},
  title =         {The General Problem of the Stability of Motion},
  year =          {1992},
}

